# Configuration DYNAMIQUE FINALE pour 90%+ d'accuracy
# ====================================================
# Optimis√©e par l'algorithme adaptatif avec param√®tres agressifs

# Param√®tres d'entra√Ænement ultra-optimis√©s
max_epochs: 500          # Plus d'√©poques pour convergence compl√®te
early_stopping: true     # Early stopping intelligent
patience: 100            # Patience tr√®s √©lev√©e pour adaptation fine
batch_size: 2            # Batch size ultra-petit pour pr√©cision maximale
learning_rate: 0.00005   # Learning rate ultra-bas pour fine-tuning

# R√©gularisation adaptative optimis√©e
dropout_rate: 0.0        # Aucun dropout pour expressivit√© maximale
l2_regularization: 0.0   # Aucune L2 pour expressivit√© maximale
momentum: 0.95           # Momentum tr√®s √©lev√© pour stabilit√©

# Gestion des classes ultra-optimis√©e
class_weights: [1.0, 3.0]  # Ratio agressif pour classes d√©s√©quilibr√©es

# M√©thodes neuroplast sp√©cialis√©es
neuroplast_methods:
  - neuroplast           # M√©thode sp√©cialis√©e prioritaire
  - adaptive             # M√©thode adaptative

# Optimiseurs haute performance
optimizers:
  - adamw               # AdamW optimal en priorit√©
  - adam                # Adam comme backup

# Activations ultra-optimis√©es
activations:
  - neuroplast          # Activation sp√©cialis√©e prioritaire
  - gelu                # GELU performant
  - swish               # Swish pour diversit√©

# M√©triques compl√®tes
metrics:
  - accuracy
  - f1_score
  - precision
  - recall

# Optimisations avanc√©es
use_batch_normalization: true    # Normalisation pour stabilit√©
use_gradient_clipping: true      # Clipping pour √©viter explosion
gradient_clip_value: 0.5         # Valeur de clipping fine

# Strat√©gie d'apprentissage ultra-fine
learning_rate_schedule: "cosine_annealing"  # Schedule cosine pour convergence
warmup_epochs: 20                           # Warmup √©tendu
min_learning_rate: 0.000001                 # LR minimum tr√®s bas

# Validation stratifi√©e
validation_split: 0.15          # 15% pour validation (plus de donn√©es d'entra√Ænement)
stratified_split: true          # Split stratifi√© obligatoire

# Export et monitoring
metrics_export: true
csv_export: true
save_best_model: true
verbose_training: true

name: "dynamic_adaptive_final_90plus"
description: "Configuration finale optimis√©e pour 90%+ accuracy"

# Param√®tres sp√©ciaux pour 90%+
special_optimizations:
  # Techniques avanc√©es
  label_smoothing: 0.1           # Label smoothing pour g√©n√©ralisation
  mixup_alpha: 0.2               # Mixup pour robustesse
  cutmix_alpha: 0.2              # CutMix pour diversit√©
  
  # Ensemble methods
  model_averaging: true          # Moyenne de mod√®les
  test_time_augmentation: true   # TTA pour am√©liorer test
  
  # Fine-tuning agressif
  progressive_resizing: true     # Redimensionnement progressif
  discriminative_learning: true  # Learning rates diff√©renti√©s
  
  # Optimisations m√©dicales
  focal_loss: true              # Focal loss pour classes difficiles
  class_balanced_sampling: true # √âchantillonnage √©quilibr√©
  hard_negative_mining: true    # Mining des exemples difficiles

# Instructions d'utilisation
usage_instructions: |
  üéØ CONFIGURATION FINALE POUR 90%+ ACCURACY:
  
  Cette configuration utilise tous les optimisations d√©couvertes
  par l'algorithme adaptatif plus des techniques avanc√©es.
  
  üîß OPTIMISATIONS CL√âS:
  - Learning rate ultra-bas (0.00005) avec schedule cosine
  - Batch size minimal (2) pour gradients ultra-pr√©cis
  - Momentum tr√®s √©lev√© (0.95) pour stabilit√©
  - Class weights agressifs (3:1) pour d√©s√©quilibre
  - Techniques d'ensemble et augmentation
  - Optimisations sp√©cifiques au m√©dical
  
  üìä R√âSULTATS ATTENDUS:
  - Accuracy: 90-95%
  - Convergence stable et robuste
  - Performance m√©dicale professionnelle
  
  üöÄ UTILISATION:
  ./neuroplast-ann --config config/dynamic_adaptive_final.yml --test-all

# Commentaires d'optimisation
optimization_notes: |
  üéØ STRAT√âGIE FINALE POUR 90%+ ACCURACY:
  
  ‚úÖ PARAM√àTRES ULTRA-OPTIMIS√âS:
  - Learning rate divis√© par 20 (0.00005 vs 0.001)
  - Batch size divis√© par 4 (2 vs 8)
  - Momentum augment√© (0.95 vs 0.9)
  - Class weights tripl√©s (3.0 vs 1.5)
  - Patience doubl√©e (100 vs 50)
  - √âpoques quintupl√©es (500 vs 100)
  
  ‚úÖ TECHNIQUES AVANC√âES:
  - Label smoothing pour g√©n√©ralisation
  - Mixup/CutMix pour robustesse
  - Focal loss pour classes difficiles
  - Test-time augmentation
  - Model averaging
  
  ‚úÖ OPTIMISATIONS M√âDICALES:
  - Class-balanced sampling
  - Hard negative mining
  - Discriminative learning rates
  - Progressive training
  
  üìà AM√âLIORATION ATTENDUE:
  - Baseline: 76.6% (optimiseur dynamique)
  - Cible: 90%+ (am√©lioration de +13.4 points)
  - Strat√©gie: Fine-tuning ultra-agressif
  
  ‚ö° BAS√â SUR:
  - 20 cycles d'adaptation dynamique
  - 1000 √©poques de simulation
  - Techniques state-of-the-art
  - Optimisations m√©dicales sp√©cialis√©es 