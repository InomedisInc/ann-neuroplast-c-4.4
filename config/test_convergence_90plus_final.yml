# Configuration FINALE OPTIMIS√âE pour 90%+ d'accuracy
# ===================================================
# Bas√©e sur test_convergence.yml mais avec param√®tres ultra-optimis√©s

# Param√®tres d'entra√Ænement ultra-optimis√©s pour 90%+
max_epochs: 300          # Plus d'√©poques pour convergence compl√®te
early_stopping: true     # Early stopping intelligent
patience: 80             # Patience tr√®s √©lev√©e pour adaptation fine
batch_size: 2            # Batch size ultra-petit pour pr√©cision maximale
learning_rate: 0.00005   # Learning rate ultra-bas pour fine-tuning

# R√©gularisation ultra-optimis√©e
dropout_rate: 0.0        # Aucun dropout pour expressivit√© maximale
l2_regularization: 0.0   # Aucune L2 pour expressivit√© maximale
momentum: 0.95           # Momentum tr√®s √©lev√© pour stabilit√©

# Gestion des classes ultra-optimis√©e pour 90%+
class_weights: [1.0, 4.0]  # Ratio tr√®s agressif pour classes d√©s√©quilibr√©es

# M√©thodes neuroplast sp√©cialis√©es
neuroplast_methods:
  - neuroplast           # M√©thode sp√©cialis√©e prioritaire
  - adaptive             # M√©thode adaptative

# Optimiseurs haute performance
optimizers:
  - adamw               # AdamW optimal en priorit√©
  - adam                # Adam comme backup

# Activations ultra-optimis√©es
activations:
  - neuroplast          # Activation sp√©cialis√©e prioritaire
  - gelu                # GELU performant
  - swish               # Swish pour diversit√©

# M√©triques compl√®tes
metrics:
  - accuracy
  - f1_score
  - precision
  - recall

name: "test_convergence_90plus_final"
description: "Configuration finale optimis√©e pour 90%+ accuracy"

# Instructions d'optimisation
optimization_notes: |
  üéØ CONFIGURATION FINALE POUR 90%+ ACCURACY:
  
  ‚úÖ OPTIMISATIONS ULTRA-AGRESSIVES:
  - Learning rate divis√© par 20 (0.00005 vs 0.001)
  - Batch size divis√© par 8 (2 vs 16)
  - Momentum augment√© (0.95 vs 0.9)
  - Class weights quadrupl√©s (4.0 vs 1.0)
  - Patience quadrupl√©e (80 vs 20)
  - √âpoques tripl√©es (300 vs 100)
  - Aucune r√©gularisation (dropout=0, L2=0)
  
  üìà STRAT√âGIE:
  - Fine-tuning ultra-pr√©cis avec learning rate minimal
  - Gradients ultra-pr√©cis avec batch size minimal
  - Stabilit√© maximale avec momentum √©lev√©
  - Compensation agressive du d√©s√©quilibre des classes
  - Convergence compl√®te avec patience √©lev√©e
  
  üéØ OBJECTIF: 90-95% accuracy
  üöÄ UTILISATION: ./neuroplast-ann --config config/test_convergence_90plus_final.yml --test-all 