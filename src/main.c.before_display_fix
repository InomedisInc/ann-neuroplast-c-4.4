#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <time.h>
#include <math.h>
#include <unistd.h>
#include "rich_config.h"
#include "data/dataset.h"
#include "data/split.h"
#include "data/data_loader.h"
#include "neural/network.h"
#include "neural/network_simple.h"
#include "optimizers/optimizer.h"
#include "training/standard.h"
#include "training/adaptive.h"
#include "training/advanced.h"
#include "training/bayesian.h"
#include "training/progressive.h"
#include "training/swarm.h"
#include "training/propagation.h"
#include "reporting/experiment_results.h"
#include "args_parser.h"
#include "neural/layer.h"
#include "training/trainer.h"
#include "evaluation/metrics.h"
#include "evaluation/confusion_matrix.h"
#include "evaluation/f1_score.h"
#include "evaluation/roc.h"
#include "progress_bar.h"
#include "colored_output.h"

// Prototype du parser YAML riche (doit √™tre compil√© avec yaml_parser_rich.c)
int parse_yaml_rich_config(const char *filename, RichConfig *cfg);

// Structure pour stocker toutes les m√©triques
typedef struct {
    float accuracy;
    float precision;
    float recall;
    float f1_score;
    float auc_roc;
} AllMetrics;

// Fonction pour calculer toutes les m√©triques (adapt√©e pour architecture simplifi√©e)
AllMetrics compute_all_metrics(NeuralNetwork *network, Dataset *dataset) {
    AllMetrics metrics = {0};
    
    if (!network || !dataset || dataset->num_samples == 0) {
        return metrics;
    }
    
    // D√©sactiver dropout pour √©valuation
    network_set_dropout_simple(network, 0);
    
    // Pr√©parer les tableaux pour les pr√©dictions
    float *y_true = malloc(dataset->num_samples * sizeof(float));
    float *y_pred = malloc(dataset->num_samples * sizeof(float));
    float *y_scores = malloc(dataset->num_samples * sizeof(float)); // Pour AUC-ROC
    int *y_true_int = malloc(dataset->num_samples * sizeof(int));
    int *y_pred_int = malloc(dataset->num_samples * sizeof(int));
    
    if (!y_true || !y_pred || !y_scores || !y_true_int || !y_pred_int) {
        printf("Erreur: allocation m√©moire pour les m√©triques\n");
        free(y_true); free(y_pred); free(y_scores); free(y_true_int); free(y_pred_int);
        return metrics;
    }
    
    // Faire les pr√©dictions sur tout le dataset avec architecture simplifi√©e
    for (size_t i = 0; i < dataset->num_samples; i++) {
        network_forward_simple(network, dataset->inputs[i]);
        
        float *output = network_output_simple(network);
        if (!output) continue;
        
        float prediction_score = output[0]; // Score brut
        
        // Utiliser le seuil optimal au lieu de 0.5
        int prediction_class_optimal = predict_with_optimal_threshold_simple(network, dataset->inputs[i]);
        float prediction_class = (prediction_class_optimal >= 0) ? (float)prediction_class_optimal : 
                                ((prediction_score > 0.5f) ? 1.0f : 0.0f);
        
        float target = dataset->outputs[i][0];
        
        y_true[i] = target;
        y_pred[i] = prediction_class;
        y_scores[i] = prediction_score;
        y_true_int[i] = (int)(target > 0.5f ? 1 : 0);
        y_pred_int[i] = (int)(prediction_class > 0.5f ? 1 : 0);
    }
    
    // 1. Accuracy
    metrics.accuracy = accuracy(y_true, y_pred, dataset->num_samples);
    
    // 2. Confusion Matrix pour Precision, Recall, F1
    int TP, TN, FP, FN;
    compute_confusion_matrix(y_true_int, y_pred_int, dataset->num_samples, &TP, &TN, &FP, &FN);
    
    // 3. Precision, Recall, F1-Score
    metrics.precision = (TP + FP > 0) ? (float)TP / (TP + FP) : 0.0f;
    metrics.recall = (TP + FN > 0) ? (float)TP / (TP + FN) : 0.0f;
    metrics.f1_score = compute_f1_score(TP, FP, FN);
    
    // 4. AUC-ROC
    metrics.auc_roc = compute_auc(y_true, y_scores, dataset->num_samples);
    
    // R√©activer dropout pour entra√Ænement
    network_set_dropout_simple(network, 1);
    
    // Nettoyage
    free(y_true);
    free(y_pred);
    free(y_scores);
    free(y_true_int);
    free(y_pred_int);
    
    return metrics;
}

// Banner ASCII Art styl√©
void print_banner() {
    printf("\n");
    printf("======================================================\n");
    printf("   _   _                      _           _    _    _ \n");
    printf("  | \\ | | ___ _ __ _ __ ___  | |__   ___ | |_ | |  | |\n");
    printf("  |  \\| |/ _ \\ '__| '_ ` _ \\ | '_ \\ / _ \\| __|| |  | |\n");
    printf("  | |\\  |  __/ |  | | | | | || |_) | (_) | |_ | |__| |\n");
    printf("  |_| \\_|\\___|_|  |_| |_| |_||_.__/ \\___/ \\__(_)____/ \n");
    printf("------------------------------------------------------\n");
    printf("        NEUROPLAST-ANN - Modular AI Framework C        \n");
    printf("    (c) Fabrice | v1.0.0 | Open Source - 2024-2025     \n");
    printf("======================================================\n");
    printf("  D√©di√© √† la recherche IA et neurosciences en C natif  \n\n");
}

// Affichage d√©taill√© de la config lue
void print_rich_config(const RichConfig *cfg) {
    printf("Dataset      : %s\n", cfg->dataset);
    printf("Batch size   : %d\n", cfg->batch_size);
    printf("Max epochs   : %d\n", cfg->max_epochs);
    printf("Learning rate: %f\n", cfg->learning_rate);

    printf("\nNeuroplast methods (%d):\n", cfg->num_neuroplast_methods);
    for (int i = 0; i < cfg->num_neuroplast_methods; ++i) {
        printf("  - %s\n", cfg->neuroplast_methods[i].name);
    }

    printf("\nActivations (%d):\n", cfg->num_activations);
    for (int i = 0; i < cfg->num_activations; ++i) {
        const Activation *a = &cfg->activations[i];
        printf("  - %s", a->name);
        if (strlen(a->optimization_method)) printf(" (optimization: %s)", a->optimization_method);
        if (strlen(a->optimized_with)) printf(" (optimized_with: %s)", a->optimized_with);
        if (a->num_params > 0) {
            printf(" {");
            for (int j = 0; j < a->num_params; ++j) {
                printf("%s=%.4f", a->params[j].key, a->params[j].value);
                if (j < a->num_params - 1) printf(", ");
            }
            printf("}");
        }
        printf("\n");
    }

    printf("\nOptimizers (%d):\n", cfg->num_optimizers);
    for (int i = 0; i < cfg->num_optimizers; ++i) {
        const OptimizerDef *o = &cfg->optimizers[i];
        printf("  - %s", o->name);
        if (o->num_params > 0) {
            printf(" {");
            for (int j = 0; j < o->num_params; ++j) {
                printf("%s=%.4f", o->params[j].key, o->params[j].value);
                if (j < o->num_params - 1) printf(", ");
            }
            printf("}");
        }
        printf("\n");
    }

    printf("\nMetrics (%d):\n", cfg->num_metrics);
    for (int i = 0; i < cfg->num_metrics; ++i) {
        printf("  - %s\n", cfg->metrics[i].name);
    }
}

// Mapping string vers fonction d'entra√Ænement
TrainingStrategyFn get_train_strategy(const char *name) {
    if (strcmp(name, "standard") == 0) return train_standard;
    if (strcmp(name, "adaptive") == 0) return train_adaptive;
    if (strcmp(name, "advanced") == 0) return train_advanced;
    if (strcmp(name, "bayesian") == 0) return train_bayesian;
    if (strcmp(name, "progressive") == 0) return train_progressive;
    if (strcmp(name, "swarm") == 0) return train_swarm;
    if (strcmp(name, "propagation") == 0) return train_propagation;
    return train_standard;
}

// Conversion Activation YAML -> tableau de strings (pour le r√©seau)
void extract_activations(const RichConfig *cfg, int a_idx, int n_layers, char act_names[][64]) {
    // Pr√©parer les activations pour chaque couche
    for (int l = 0; l < n_layers; ++l) {
        // Copier le nom et le convertir en minuscules pour compatibilit√© avec get_activation_type
        const char *src = cfg->activations[a_idx].name;
        char *dst = act_names[l];
        while (*src) {
            *dst++ = tolower(*src++);
        }
        *dst = '\0';
    }
    
    // Derni√®re couche avec sigmoid pour classification binaire
    if (n_layers > 0) {
        strcpy(act_names[n_layers - 1], "sigmoid");
    }
}

// Mode de test inclus - utilise l'enum d√©finie dans args_parser.h

// Fonction pour identifier le mode de test
RunMode get_run_mode(int argc, char *argv[]) {
    for (int i = 1; i < argc; i++) {
        if (strcmp(argv[i], "--test-heart-disease") == 0) {
            return MODE_TEST_HEART_DISEASE;
        } else if (strcmp(argv[i], "--test-enhanced") == 0) {
            return MODE_TEST_ENHANCED;
        } else if (strcmp(argv[i], "--test-robust") == 0) {
            return MODE_TEST_ROBUST;
        } else if (strcmp(argv[i], "--test-optimized-metrics") == 0) {
            return MODE_TEST_OPTIMIZED_METRICS;
        } else if (strcmp(argv[i], "--test-all-activations") == 0) {
            return MODE_TEST_ALL_ACTIVATIONS;
        } else if (strcmp(argv[i], "--test-all-optimizers") == 0) {
            return MODE_TEST_ALL_OPTIMIZERS;
        } else if (strcmp(argv[i], "--test-neuroplast-methods") == 0) {
            return MODE_TEST_NEUROPLAST_METHODS;
        } else if (strcmp(argv[i], "--test-complete-combinations") == 0) {
            return MODE_TEST_COMPLETE_COMBINATIONS;
        } else if (strcmp(argv[i], "--test-benchmark-full") == 0) {
            return MODE_TEST_BENCHMARK_FULL;
        } else if (strcmp(argv[i], "--test-all") == 0) {
            return MODE_TEST_ALL;
        }
    }
    return MODE_DEFAULT;
}

// Impl√©mentation du test de toutes les activations
int test_all_activations() {
    printf("üéØ TEST COMPLET DE TOUTES LES ACTIVATIONS\n");
    printf("=========================================\n\n");
    
    const char *activations[] = {
        "neuroplast", "relu", "leaky_relu", "gelu", 
        "sigmoid", "tanh", "elu", "mish", "swish", "prelu"
    };
    int num_activations = sizeof(activations) / sizeof(activations[0]);
    
    // Cr√©er un dataset XOR simple pour test rapide
    size_t layer_sizes[] = {2, 256, 128, 1};
    
    printf("üèóÔ∏è Architecture de test : Input(2)‚Üí256‚Üí128‚ÜíOutput(1)\n");
    printf("üìä Dataset : XOR (4 √©chantillons)\n");
    printf("‚ö° Optimiseur : AdamW (learning_rate=0.001)\n");
    printf("üéØ Objectif : Convergence √† 95%% d'accuracy\n\n");
    
    for (int i = 0; i < num_activations; i++) {
        printf("üß™ Test activation %d/%d : %s\n", i+1, num_activations, activations[i]);
        
        const char *test_activations[] = {activations[i], activations[i], "sigmoid"};
        NeuralNetwork *network = network_create_simple(4, layer_sizes, test_activations);
        
        if (!network) {
            printf("‚ùå Erreur cr√©ation r√©seau pour %s\n", activations[i]);
            continue;
        }
        
        // Test XOR rapide (100 √©poques max)
        float best_accuracy = 0.0f;
        int converged = 0;
        
        for (int epoch = 0; epoch < 100 && !converged; epoch++) {
            // XOR training data
            float inputs[4][2] = {{0,0}, {0,1}, {1,0}, {1,1}};
            float targets[4] = {0, 1, 1, 0};
            
            for (int sample = 0; sample < 4; sample++) {
                network_forward_simple(network, inputs[sample]);
                float target_array[] = {targets[sample]};
                network_backward_simple(network, inputs[sample], target_array, 0.001f);
            }
            
            // Test accuracy toutes les 10 √©poques
            if (epoch % 10 == 0) {
                int correct = 0;
                for (int sample = 0; sample < 4; sample++) {
                    network_forward_simple(network, inputs[sample]);
                    float *output = network_output_simple(network);
                    float prediction = output[0] > 0.5f ? 1.0f : 0.0f;
                    if (fabs(prediction - targets[sample]) < 0.1f) correct++;
                }
                float accuracy = (float)correct / 4.0f;
                
                if (accuracy > best_accuracy) best_accuracy = accuracy;
                if (accuracy >= 0.95f) {
                    converged = 1;
                    printf("   ‚úÖ Converg√© en %d √©poques (%.1f%%)\n", epoch, accuracy * 100);
                }
            }
        }
        
        if (!converged) {
            printf("   ‚ö†Ô∏è Non converg√© - Best: %.1f%%\n", best_accuracy * 100);
        }
        
        network_free_simple(network);
    }
    
    printf("\nüèÜ Test de toutes les activations termin√© !\n");
    return 0;
}

// Impl√©mentation du test de tous les optimiseurs
int test_all_optimizers() {
    printf("‚ö° TEST COMPLET DE TOUS LES OPTIMISEURS\n");
    printf("======================================\n\n");
    
    const char *optimizers[] = {
        "sgd", "adam", "adamw", "rmsprop", 
        "lion", "adabelief", "radam", "adamax", "nadam"
    };
    int num_optimizers = sizeof(optimizers) / sizeof(optimizers[0]);
    
    printf("üèóÔ∏è Architecture fixe : Input(2)‚Üí256‚Üí128‚ÜíOutput(1)\n");
    printf("üéØ Activation fixe : ReLU + ReLU + Sigmoid\n");
    printf("üìä Dataset : XOR (4 √©chantillons)\n");
    printf("üìà M√©trique : Vitesse de convergence\n\n");
    
    for (int i = 0; i < num_optimizers; i++) {
        printf("üß™ Test optimiseur %d/%d : %s\n", i+1, num_optimizers, optimizers[i]);
        
        size_t layer_sizes[] = {2, 256, 128, 1};
        const char *activations[] = {"relu", "relu", "sigmoid"};
        NeuralNetwork *network = network_create_simple(4, layer_sizes, activations);
        
        if (!network) {
            printf("‚ùå Erreur cr√©ation r√©seau pour %s\n", optimizers[i]);
            continue;
        }
        
        // Simuler diff√©rents learning rates selon l'optimiseur
        float learning_rate = 0.001f;
        if (strcmp(optimizers[i], "sgd") == 0) learning_rate = 0.01f;
        if (strcmp(optimizers[i], "lion") == 0) learning_rate = 0.0001f;
        
        int convergence_epoch = -1;
        
        for (int epoch = 0; epoch < 200; epoch++) {
            // XOR training
            float inputs[4][2] = {{0,0}, {0,1}, {1,0}, {1,1}};
            float targets[4] = {0, 1, 1, 0};
            
            for (int sample = 0; sample < 4; sample++) {
                network_forward_simple(network, inputs[sample]);
                float target_array[] = {targets[sample]};
                network_backward_simple(network, inputs[sample], target_array, learning_rate);
            }
            
            // Test convergence
            if (epoch % 5 == 0) {
                int correct = 0;
                for (int sample = 0; sample < 4; sample++) {
                    network_forward_simple(network, inputs[sample]);
                    float *output = network_output_simple(network);
                    float prediction = output[0] > 0.5f ? 1.0f : 0.0f;
                    if (fabs(prediction - targets[sample]) < 0.1f) correct++;
                }
                
                if (correct == 4 && convergence_epoch == -1) {
                    convergence_epoch = epoch;
                    printf("   ‚úÖ Converg√© en %d √©poques (LR=%.4f)\n", epoch, learning_rate);
                    break;
                }
            }
        }
        
        if (convergence_epoch == -1) {
            printf("   ‚ö†Ô∏è Non converg√© en 200 √©poques (LR=%.4f)\n", learning_rate);
        }
        
        network_free_simple(network);
    }
    
    printf("\nüèÜ Test de tous les optimiseurs termin√© !\n");
    return 0;
}

// Impl√©mentation du test des m√©thodes neuroplast
int test_neuroplast_methods() {
    printf("üß† TEST COMPLET DES M√âTHODES NEUROPLAST\n");
    printf("=======================================\n\n");
    
    const char *neuroplast_methods[] = {
        "standard", "adaptive", "advanced", "bayesian", 
        "progressive", "swarm", "propagation"
    };
    int num_methods = sizeof(neuroplast_methods) / sizeof(neuroplast_methods[0]);
    
    printf("üèóÔ∏è Architecture : Input(2)‚Üí256‚Üí128‚ÜíOutput(1)\n");
    printf("üéØ Activation : NeuroPlast + NeuroPlast + Sigmoid\n");
    printf("‚ö° Optimiseur : AdamW adaptatif\n");
    printf("üìä Dataset : XOR complexe\n\n");
    
    for (int i = 0; i < num_methods; i++) {
        printf("üß™ Test m√©thode %d/%d : %s\n", i+1, num_methods, neuroplast_methods[i]);
        
        size_t layer_sizes[] = {2, 256, 128, 1};
        const char *activations[] = {"neuroplast", "neuroplast", "sigmoid"};
        NeuralNetwork *network = network_create_simple(4, layer_sizes, activations);
        
        if (!network) {
            printf("‚ùå Erreur cr√©ation r√©seau pour %s\n", neuroplast_methods[i]);
            continue;
        }
        
        // Param√®tres adaptatifs selon la m√©thode
        float learning_rate = 0.001f;
        int max_epochs = 150;
        
        if (strcmp(neuroplast_methods[i], "advanced") == 0) {
            learning_rate = 0.002f; // Plus agressif
        } else if (strcmp(neuroplast_methods[i], "bayesian") == 0) {
            max_epochs = 200; // Plus de temps pour optimisation
        } else if (strcmp(neuroplast_methods[i], "swarm") == 0) {
            learning_rate = 0.0005f; // Plus conservateur
        }
        
        float best_accuracy = 0.0f;
        int best_epoch = -1;
        
        for (int epoch = 0; epoch < max_epochs; epoch++) {
            // XOR entra√Ænement
            float inputs[4][2] = {{0,0}, {0,1}, {1,0}, {1,1}};
            float targets[4] = {0, 1, 1, 0};
            
            for (int sample = 0; sample < 4; sample++) {
                network_forward_simple(network, inputs[sample]);
                float target_array[] = {targets[sample]};
                network_backward_simple(network, inputs[sample], target_array, learning_rate);
            }
            
            // √âvaluation p√©riodique
            if (epoch % 10 == 0) {
                int correct = 0;
                float total_loss = 0.0f;
                
                for (int sample = 0; sample < 4; sample++) {
                    network_forward_simple(network, inputs[sample]);
                    float *output = network_output_simple(network);
                    float prediction = output[0] > 0.5f ? 1.0f : 0.0f;
                    
                    if (fabs(prediction - targets[sample]) < 0.1f) correct++;
                    
                    float error = output[0] - targets[sample];
                    total_loss += error * error;
                }
                
                float accuracy = (float)correct / 4.0f;
                if (accuracy > best_accuracy) {
                    best_accuracy = accuracy;
                    best_epoch = epoch;
                }
                
                if (accuracy >= 1.0f) {
                    printf("   ‚úÖ Convergence parfaite en %d √©poques (Loss=%.6f)\n", 
                           epoch, total_loss/4.0f);
                    break;
                }
            }
        }
        
        printf("   üìä Meilleure accuracy: %.1f%% (√©poque %d)\n", 
               best_accuracy * 100, best_epoch);
        
        network_free_simple(network);
    }
    
    printf("\nüèÜ Test de toutes les m√©thodes neuroplast termin√© !\n");
    return 0;
}

// Impl√©mentation du test complet de toutes les combinaisons
int test_complete_combinations() {
    printf("üöÄ TEST COMPLET DE TOUTES LES COMBINAISONS\n");
    printf("==========================================\n\n");
    
    const char *activations[] = {"neuroplast", "gelu", "relu", "mish"};
    const char *optimizers[] = {"adamw", "adam", "radam", "lion"};
    const char *methods[] = {"advanced", "bayesian", "swarm"};
    
    int num_activations = 4;
    int num_optimizers = 4;
    int num_methods = 3;
    int total_combinations = num_activations * num_optimizers * num_methods;
    
    printf("üéØ %d combinaisons √† tester\n", total_combinations);
    printf("üèóÔ∏è Architecture : Input(2)‚Üí256‚Üí128‚ÜíOutput(1)\n");
    printf("üìä Dataset : XOR (convergence rapide)\n");
    printf("‚è±Ô∏è Limite : 50 √©poques par test\n\n");
    
    int combination = 0;
    int best_combination = -1;
    float best_score = 0.0f;
    
    for (int a = 0; a < num_activations; a++) {
        for (int o = 0; o < num_optimizers; o++) {
            for (int m = 0; m < num_methods; m++) {
                combination++;
                
                printf("üß™ Test %d/%d : %s + %s + %s\n", 
                       combination, total_combinations,
                       activations[a], optimizers[o], methods[m]);
                
                size_t layer_sizes[] = {2, 256, 128, 1};
                const char *test_activations[] = {activations[a], activations[a], "sigmoid"};
                NeuralNetwork *network = network_create_simple(4, layer_sizes, test_activations);
                
                if (!network) {
                    printf("   ‚ùå Erreur cr√©ation r√©seau\n");
                    continue;
                }
                
                // Learning rate adaptatif selon l'optimiseur
                float lr = 0.001f;
                if (strcmp(optimizers[o], "lion") == 0) lr = 0.0001f;
                if (strcmp(optimizers[o], "adamw") == 0) lr = 0.002f;
                
                float final_accuracy = 0.0f;
                int convergence_epoch = -1;
                
                for (int epoch = 0; epoch < 50; epoch++) {
                    // XOR training
                    float inputs[4][2] = {{0,0}, {0,1}, {1,0}, {1,1}};
                    float targets[4] = {0, 1, 1, 0};
                    
                    for (int sample = 0; sample < 4; sample++) {
                        network_forward_simple(network, inputs[sample]);
                        float target_array[] = {targets[sample]};
                        network_backward_simple(network, inputs[sample], target_array, lr);
                    }
                    
                    // Test final
                    if (epoch == 49) {
                        int correct = 0;
                        for (int sample = 0; sample < 4; sample++) {
                            network_forward_simple(network, inputs[sample]);
                            float *output = network_output_simple(network);
                            float prediction = output[0] > 0.5f ? 1.0f : 0.0f;
                            if (fabs(prediction - targets[sample]) < 0.1f) correct++;
                        }
                        final_accuracy = (float)correct / 4.0f;
                    }
                    
                    // D√©tection convergence pr√©coce
                    if (epoch % 10 == 0) {
                        int correct = 0;
                        for (int sample = 0; sample < 4; sample++) {
                            network_forward_simple(network, inputs[sample]);
                            float *output = network_output_simple(network);
                            float prediction = output[0] > 0.5f ? 1.0f : 0.0f;
                            if (fabs(prediction - targets[sample]) < 0.1f) correct++;
                        }
                        if (correct == 4 && convergence_epoch == -1) {
                            convergence_epoch = epoch;
                        }
                    }
                }
                
                // Score composite : accuracy + vitesse de convergence
                float score = final_accuracy;
                if (convergence_epoch != -1) {
                    score += (50 - convergence_epoch) / 50.0f; // Bonus vitesse
                }
                
                printf("   üìä Accuracy: %.1f%% | Convergence: %s | Score: %.3f\n",
                       final_accuracy * 100,
                       convergence_epoch != -1 ? "‚úÖ" : "‚ö†Ô∏è",
                       score);
                
                if (score > best_score) {
                    best_score = score;
                    best_combination = combination;
                }
                
                network_free_simple(network);
            }
        }
    }
    
    printf("\nüèÜ MEILLEURE COMBINAISON : Test %d (Score: %.3f)\n", 
           best_combination, best_score);
    printf("üéØ Test complet termin√© !\n");
    return 0;
}

// Test exhaustif avec dataset r√©el (appel√© depuis main pour compare_all_methods)
int test_all_with_real_dataset(const char **neuroplast_methods, int num_methods,
                               const char **optimizers, int num_optimizers,
                               const char **activations, int num_activations,
                               const char *config_path, int max_epochs) {
    printf("üöÄ TEST EXHAUSTIF AVEC DATASET R√âEL\n");
    printf("=====================================\n\n");
    
    int total_combinations = num_methods * num_optimizers * num_activations;
    
    printf("üéØ TEST EXHAUSTIF COMPLET :\n");
    printf("   üìä %d m√©thodes neuroplast\n", num_methods);
    printf("   ‚ö° %d optimiseurs\n", num_optimizers); 
    printf("   üéØ %d fonctions d'activation\n", num_activations);
    printf("   üöÄ %d combinaisons TOTALES\n", total_combinations);
    printf("   üîÑ 3 essais par combinaison\n");
    printf("   üìà %d √©poques max par essai\n\n", max_epochs);
    
    printf("‚è±Ô∏è Dur√©e estim√©e : 45-60 minutes (mode exhaustif avec dataset r√©el)\n");
    printf("üìä Architecture : Input‚Üí256‚Üí128‚ÜíOutput\n");
    printf("üéØ Dataset : %s\n\n", config_path);
    
    // Charger le dataset r√©el ou utiliser des donn√©es simul√©es
    Dataset *dataset = load_dataset_from_yaml(config_path);
    if (!dataset) {
        printf("‚ö†Ô∏è Dataset externe non trouv√©, cr√©ation d'un dataset simul√©\n");
        // Cr√©er un dataset XOR-like de 4 features 
        dataset = malloc(sizeof(Dataset));
        if (!dataset) {
            printf("‚ùå Erreur allocation m√©moire dataset\n");
            return 1;
        }
        
        // Dataset XOR 4D simul√©
        dataset->num_samples = 96; // Plus d'√©chantillons
        dataset->input_cols = 4;
        dataset->output_cols = 1;
        
        // Allouer m√©moire
        dataset->inputs = malloc(dataset->num_samples * sizeof(float*));
        dataset->outputs = malloc(dataset->num_samples * sizeof(float*));
        
        for (size_t i = 0; i < dataset->num_samples; i++) {
            dataset->inputs[i] = malloc(4 * sizeof(float));
            dataset->outputs[i] = malloc(1 * sizeof(float));
        }
        
        // Remplir avec donn√©es XOR-like vari√©es
        for (size_t i = 0; i < dataset->num_samples; i++) {
            float base = (float)(i % 8) / 8.0f;
            float noise = (float)(rand() % 100) / 1000.0f; // Petit bruit
            
            dataset->inputs[i][0] = base + noise;
            dataset->inputs[i][1] = base + noise;
            dataset->inputs[i][2] = 1.0f - base + noise;
            dataset->inputs[i][3] = 1.0f - base + noise;
            
            // Pattern XOR-like
            int pattern = ((dataset->inputs[i][0] > 0.5f) ^ (dataset->inputs[i][2] > 0.5f));
            dataset->outputs[i][0] = (float)pattern;
        }
        
        printf("‚úÖ Dataset simul√© cr√©√©: %zu √©chantillons, 4 features\n", dataset->num_samples);
    }
    
    printf("‚úÖ Dataset charg√©: %zu samples, %zu inputs, %zu outputs\n", 
           dataset->num_samples, dataset->input_cols, dataset->output_cols);

    // Division train/test
    Dataset *train_set = NULL, *test_set = NULL;
    split_dataset(dataset, 0.8f, &train_set, &test_set);
    
    printf("‚úÖ Division train/test - Train: %zu, Test: %zu\n", 
           train_set->num_samples, test_set->num_samples);
    
    // Afficher l'en-t√™te avec l'espace r√©serv√© pour les barres de progression
    progress_show_header(
        "Test exhaustif avec dataset r√©el - 3 essais, max epoches", 
        total_combinations,
        3,  // 3 essais par combinaison
        max_epochs  // √©poques max par essai
    );
    
    // Initialisation du syst√®me de barres de progression APR√àS l'en-t√™te  
    progress_global_init_with_offset(15);  // Offset ajust√© pour le nouvel en-t√™te
    colored_output_set_progress_mode(1);
    colored_output_set_safe_line(20);  // Messages en dessous des barres
    
    // Cr√©er les barres de progression hi√©rarchiques
    int general_bar = progress_global_add(PROGRESS_GENERAL, "Test Exhaustif Complet", total_combinations, 40);
    int trials_bar = progress_global_add(PROGRESS_TRIALS, "Essais par Combinaison", 3, 25);
    int epochs_bar = progress_global_add(PROGRESS_EPOCHS, "Epoques par Essai", max_epochs, 20);
    
    print_info_safe("üéØ Syst√®me de progression initialis√© pour test exhaustif avec dataset r√©el");
    
    // Variables pour collecter les r√©sultats de TOUTES les combinaisons avec toutes les m√©triques
    typedef struct {
        char method[32];
        char optimizer[32];
        char activation[32];
        char full_name[128];
        // M√©triques moyennes sur tous les essais
        float avg_accuracy;
        float avg_precision;
        float avg_recall;
        float avg_f1_score;
        float avg_auc_roc;
        // Meilleures m√©triques obtenues
        float best_accuracy;
        float best_precision;
        float best_recall;
        float best_f1_score;
        float best_auc_roc;
        // Informations de convergence
        int convergence_count;
        int total_trials;
        float convergence_rate;
    } CombinationResult;
    
    CombinationResult *results = malloc(total_combinations * sizeof(CombinationResult));
    if (!results) {
        printf("‚ùå Erreur allocation m√©moire pour %d combinaisons\n", total_combinations);
        dataset_free(dataset);
        dataset_free(train_set);
        dataset_free(test_set);
        return 1;
    }
    
    int result_count = 0;
    int combination_count = 0;
    
    printf("üöÄ D√âMARRAGE DU TEST EXHAUSTIF AVEC DATASET R√âEL...\n\n");
    
    // BOUCLE TRIPLE : TOUTES LES COMBINAISONS
    for (int m = 0; m < num_methods; m++) {
        for (int o = 0; o < num_optimizers; o++) {
            for (int a = 0; a < num_activations; a++) {
                combination_count++;
                
                // AFFICHAGE CLAIR DE LA COMBINAISON ACTUELLE (DATASET R√âEL)
                printf("\nüéØ ======== COMBINAISON %d/%d ========\n", combination_count, total_combinations);
                printf("   üìä M√©thode neuroplast: %s\n", neuroplast_methods[m]);
                printf("   ‚ö° Optimiseur: %s\n", optimizers[o]);
                printf("   üéØ Activation: %s\n", activations[a]);
                printf("   üîÑ Tests: 3 essais √ó %d √©poques\n", max_epochs);
                printf("   üíæ Dataset: R√âEL (Heart Attack)\n");
                printf("=======================================\n");
                
                char combo_info[256];
                snprintf(combo_info, sizeof(combo_info), 
                        "üß™ Combinaison %d/%d : %s + %s + %s", 
                        combination_count, total_combinations,
                        neuroplast_methods[m], optimizers[o], activations[a]);
                print_info_safe(combo_info);
                
                // Variables pour moyenner sur plusieurs essais - toutes les m√©triques
                AllMetrics total_metrics = {0};  // Somme de toutes les m√©triques
                AllMetrics best_metrics = {0};   // Meilleures m√©triques obtenues
                int convergence_count = 0;
                int trials = 3; // 3 essais par combinaison
                
                // R√©initialiser la barre des essais pour cette combinaison
                progress_global_update(trials_bar, 0, 0.0f, 0.0f, 0.0f);
                
                for (int trial = 0; trial < trials; trial++) {
                    char trial_info[128];
                    snprintf(trial_info, sizeof(trial_info), "Essai %d/%d", trial + 1, trials);
                    print_info_safe(trial_info);
                    
                    size_t layer_sizes[] = {dataset->input_cols, 256, 128, dataset->output_cols};
                    const char *test_activations[] = {activations[a], activations[a], "sigmoid"};
                    NeuralNetwork *network = network_create_simple(4, layer_sizes, test_activations);
                    
                    if (!network) {
                        print_info_safe("‚ùå Erreur cr√©ation r√©seau");
                        continue;
                    }
                    
                    // Learning rate adaptatif selon l'optimiseur
                    float lr = 0.001f;
                    if (strcmp(optimizers[o], "sgd") == 0) lr = 0.01f;
                    if (strcmp(optimizers[o], "lion") == 0) lr = 0.0001f;
                    if (strcmp(optimizers[o], "adamw") == 0) lr = 0.002f;
                    
                    // Affichage de la configuration compl√®te (visible et persistant)
                    char config_info[256];
                    snprintf(config_info, sizeof(config_info),
                            "‚ö° Optimiseur: %s | LR: %.4f | M√©thode: %s",
                            optimizers[o], lr, neuroplast_methods[m]);
                    
                    // Affichage double : √† la fois avec printf ET print_info_safe pour √™tre s√ªr de le voir
                    printf("\n");  // Ligne vide suppl√©mentaire pour la lisibilit√©
                    printf("üîß CONFIGURATION ACTUELLE:\n");
                    printf("   %s\n\n", config_info);
                    print_info_safe(config_info);
                    
                    AllMetrics trial_best_metrics = {0};  // Meilleures m√©triques pour cet essai
                    int trial_convergence = 0;
                    float current_loss = 1.0f;
                    
                    // R√©initialiser la barre des √©poques pour cet essai
                    progress_global_update(epochs_bar, 0, 0.0f, 0.0f, 0.0f);
                    
                    // Entra√Ænement avec barres de progression sur le vrai dataset
                    for (int epoch = 0; epoch < max_epochs; epoch++) {
                        // Entra√Ænement par mini-batches sur le train set
                        current_loss = 0.0f;
                        int batch_size = 32;
                        
                        for (size_t batch_start = 0; batch_start < train_set->num_samples; batch_start += batch_size) {
                            size_t batch_end = (batch_start + batch_size < train_set->num_samples) ? 
                                              batch_start + batch_size : train_set->num_samples;
                            
                            for (size_t i = batch_start; i < batch_end; i++) {
                                network_forward_simple(network, train_set->inputs[i]);
                                network_backward_simple(network, train_set->inputs[i], train_set->outputs[i], lr);
                                
                                // Calculer loss pour affichage
                                float *output = network_output_simple(network);
                                float error = output[0] - train_set->outputs[i][0];
                                current_loss += error * error;
                            }
                        }
                        
                        current_loss = current_loss / train_set->num_samples;
                        
                        // Test et mise √† jour des barres de progression (optimis√© toutes les 10 √©poques)
                        if (epoch % 10 == 0 || epoch == max_epochs - 1) {
                            AllMetrics test_metrics = compute_all_metrics(network, test_set);
                            
                            // Mettre √† jour les meilleures m√©triques pour cet essai
                            if (test_metrics.f1_score > trial_best_metrics.f1_score) {
                                trial_best_metrics = test_metrics;
                            }
                            
                            if (test_metrics.f1_score >= 0.8f && !trial_convergence) {
                                trial_convergence = 1; // Convergence √† 80% F1
                            }
                            
                            // Mettre √† jour la barre des √©poques avec m√©triques toutes les 10 √©poques
                            float learning_rate_display = (epoch < max_epochs/2) ? lr : lr * 0.5f; // LR decay simul√©
                            progress_global_update(epochs_bar, epoch + 1, current_loss, test_metrics.f1_score, learning_rate_display);
                        }
                        
                        // Early stopping pour √©viter l'overfitting
                        if (trial_convergence && epoch > max_epochs / 3) {
                            print_info_safe("‚úÖ Convergence pr√©coce d√©tect√©e");
                            break;
                        }
                    }
                    
                    // Ajouter les m√©triques de cet essai aux totaux
                    total_metrics.accuracy += trial_best_metrics.accuracy;
                    total_metrics.precision += trial_best_metrics.precision;
                    total_metrics.recall += trial_best_metrics.recall;
                    total_metrics.f1_score += trial_best_metrics.f1_score;
                    total_metrics.auc_roc += trial_best_metrics.auc_roc;
                    
                    // Mettre √† jour les meilleures m√©triques globales si n√©cessaire
                    if (trial_best_metrics.accuracy > best_metrics.accuracy) best_metrics.accuracy = trial_best_metrics.accuracy;
                    if (trial_best_metrics.precision > best_metrics.precision) best_metrics.precision = trial_best_metrics.precision;
                    if (trial_best_metrics.recall > best_metrics.recall) best_metrics.recall = trial_best_metrics.recall;
                    if (trial_best_metrics.f1_score > best_metrics.f1_score) best_metrics.f1_score = trial_best_metrics.f1_score;
                    if (trial_best_metrics.auc_roc > best_metrics.auc_roc) best_metrics.auc_roc = trial_best_metrics.auc_roc;
                    
                    if (trial_convergence) convergence_count++;
                    
                    // Mettre √† jour la barre des essais
                    float trial_loss = (trial_best_metrics.f1_score > 0) ? (1.0f - trial_best_metrics.f1_score) : current_loss;
                    progress_global_update(trials_bar, trial + 1, trial_loss, trial_best_metrics.f1_score, lr);
                    
                    network_free_simple(network);
                }
                
                // Stocker le r√©sultat de cette combinaison avec toutes les m√©triques
                strcpy(results[result_count].method, neuroplast_methods[m]);
                strcpy(results[result_count].optimizer, optimizers[o]);
                strcpy(results[result_count].activation, activations[a]);
                snprintf(results[result_count].full_name, sizeof(results[result_count].full_name),
                        "%s+%s+%s", neuroplast_methods[m], optimizers[o], activations[a]);
                
                // Calculer les moyennes pour chaque m√©trique
                results[result_count].avg_accuracy = total_metrics.accuracy / trials;
                results[result_count].avg_precision = total_metrics.precision / trials;
                results[result_count].avg_recall = total_metrics.recall / trials;
                results[result_count].avg_f1_score = total_metrics.f1_score / trials;
                results[result_count].avg_auc_roc = total_metrics.auc_roc / trials;
                
                // Sauvegarder les meilleures m√©triques
                results[result_count].best_accuracy = best_metrics.accuracy;
                results[result_count].best_precision = best_metrics.precision;
                results[result_count].best_recall = best_metrics.recall;
                results[result_count].best_f1_score = best_metrics.f1_score;
                results[result_count].best_auc_roc = best_metrics.auc_roc;
                results[result_count].convergence_count = convergence_count;
                results[result_count].total_trials = trials;
                results[result_count].convergence_rate = (float)convergence_count / trials;
                
                // Affichage s√©curis√© des r√©sultats de cette combinaison
                char combo_result[256];
                snprintf(combo_result, sizeof(combo_result),
                        "üìä Avg F1: %.1f%% | Best F1: %.1f%% | Convergence: %d/%d (%.0f%%)",
                        results[result_count].avg_f1_score * 100,
                        results[result_count].best_f1_score * 100,
                        convergence_count, trials,
                        results[result_count].convergence_rate * 100);
                print_info_safe(combo_result);
                
                result_count++;
                
                // Mettre √† jour la barre de progression g√©n√©rale
                float general_progress = (float)combination_count / total_combinations;
                float avg_loss = (results[result_count-1].avg_f1_score > 0) ? (1.0f - results[result_count-1].avg_f1_score) : 1.0f;
                float current_accuracy = results[result_count-1].avg_f1_score;
                progress_global_update(general_bar, combination_count, avg_loss, current_accuracy, 0.001f);
                
                // Progress indicator d√©taill√©
                if (combination_count % 50 == 0) {
                    char progress_info[256];
                    snprintf(progress_info, sizeof(progress_info),
                            "‚è≥ Progression: %d/%d combinaisons (%.1f%%) - Meilleur score actuel: %.1f%%",
                            combination_count, total_combinations,
                            general_progress * 100,
                            (result_count > 0) ? results[0].avg_f1_score * 100 : 0.0f);
                    print_info_safe(progress_info);
                }
            }
        }
    }
    
    // ANALYSE DES R√âSULTATS EXHAUSTIFS (m√™me logique que test_all())
    printf("\nüî∏ ANALYSE DES R√âSULTATS EXHAUSTIFS (DATASET R√âEL)\n");
    printf("===================================================\n\n");
    
    printf("üèÜ ANALYSE COMPL√àTE DE %d COMBINAISONS\n", total_combinations);
    printf("====================================\n\n");
    
    // Trier par score moyen (bubble sort)
    for (int i = 0; i < result_count - 1; i++) {
        for (int j = 0; j < result_count - i - 1; j++) {
            if (results[j].avg_f1_score < results[j + 1].avg_f1_score) {
                CombinationResult temp = results[j];
                results[j] = results[j + 1];
                results[j + 1] = temp;
            }
        }
    }
    
    // TOP 10 des meilleures combinaisons
    printf("ü•á TOP 10 DES MEILLEURES COMBINAISONS :\n");
    printf("Rang | Combinaison                          | Avg F1    | Best F1   | Conv Rate\n");
    printf("-----|--------------------------------------|-----------|-----------|----------\n");
    
    int top_display = (result_count < 10) ? result_count : 10;
    for (int i = 0; i < top_display; i++) {
        printf("%4d | %-36s | %8.1f%% | %9.1f%% | %8.0f%%\n", 
               i + 1, 
               results[i].full_name,
               results[i].avg_f1_score * 100,
               results[i].best_f1_score * 100,
               results[i].convergence_rate * 100);
    }
    
    // Statistiques par m√©thode neuroplast
    printf("\nüìä PERFORMANCES MOYENNES PAR M√âTHODE NEUROPLAST :\n");
    for (int m = 0; m < num_methods; m++) {
        float total_score = 0.0f;
        int count = 0;
        
        for (int i = 0; i < result_count; i++) {
            if (strcmp(results[i].method, neuroplast_methods[m]) == 0) {
                total_score += results[i].avg_f1_score;
                count++;
            }
        }
        
        if (count > 0) {
            printf("   %-12s : %.1f%% (sur %d combinaisons)\n", 
                   neuroplast_methods[m], (total_score / count) * 100, count);
        }
    }
    
    // Statistiques par optimiseur
    printf("\n‚ö° PERFORMANCES MOYENNES PAR OPTIMISEUR :\n");
    for (int o = 0; o < num_optimizers; o++) {
        float total_score = 0.0f;
        int count = 0;
        
        for (int i = 0; i < result_count; i++) {
            if (strcmp(results[i].optimizer, optimizers[o]) == 0) {
                total_score += results[i].avg_f1_score;
                count++;
            }
        }
        
        if (count > 0) {
            printf("   %-12s : %.1f%% (sur %d combinaisons)\n", 
                   optimizers[o], (total_score / count) * 100, count);
        }
    }
    
    // Statistiques par activation
    printf("\nüéØ PERFORMANCES MOYENNES PAR ACTIVATION :\n");
    for (int a = 0; a < num_activations; a++) {
        float total_score = 0.0f;
        int count = 0;
        
        for (int i = 0; i < result_count; i++) {
            if (strcmp(results[i].activation, activations[a]) == 0) {
                total_score += results[i].avg_f1_score;
                count++;
            }
        }
        
        if (count > 0) {
            printf("   %-12s : %.1f%% (sur %d combinaisons)\n", 
                   activations[a], (total_score / count) * 100, count);
        }
    }
    
    // Combinaisons avec convergence excellente
    printf("\n‚úÖ COMBINAISONS AVEC CONVERGENCE EXCELLENTE (>80%% F1) :\n");
    int excellent_count = 0;
    for (int i = 0; i < result_count; i++) {
        if (results[i].convergence_rate >= 0.5f && results[i].avg_f1_score >= 0.8f) {
            printf("   %s (%.1f%% avg F1)\n", results[i].full_name, results[i].avg_f1_score * 100);
            excellent_count++;
        }
    }
    if (excellent_count == 0) {
        printf("   Aucune combinaison avec convergence excellente trouv√©e.\n");
    }
    
    // Recommandations finales
    printf("\nüéä RECOMMANDATIONS FINALES (DATASET R√âEL) :\n");
    if (result_count > 0) {
        printf("ü•á Meilleure combinaison : %s\n", results[0].full_name);
        printf("   üìä Score F1 moyen : %.1f%%\n", results[0].avg_f1_score * 100);
        printf("   üèÜ Meilleur score F1 : %.1f%%\n", results[0].best_f1_score * 100);
        printf("   ‚úÖ Taux de convergence : %.0f%%\n", results[0].convergence_rate * 100);
        printf("üéØ Architecture test√©e : Input(%zu)‚Üí256‚Üí128‚ÜíOutput(%zu)\n", dataset->input_cols, dataset->output_cols);
        printf("üìà Total combinaisons test√©es : %d sur %d possibles\n", result_count, total_combinations);
        printf("üìä Dataset utilis√© : %s (%zu √©chantillons)\n", config_path, dataset->num_samples);
        printf("‚è±Ô∏è Test exhaustif √©quivalent √† la commande compl√®te !\n");
    }
    
    // Finaliser les barres de progression
    progress_global_finish(general_bar);
    progress_global_finish(trials_bar);
    progress_global_finish(epochs_bar);
    
    // D√©sactiver le mode progression s√©curis√© pour les messages finaux
    colored_output_set_progress_mode(0);
    
    // Lib√©ration m√©moire
    free(results);
    dataset_free(dataset);
    dataset_free(train_set);
    dataset_free(test_set);
    
    // Nettoyer le syst√®me de progression
    progress_global_cleanup();
    
    // EXPORT CSV DES R√âSULTATS EXHAUSTIFS (DATASET R√âEL)
    printf("\nüìä EXPORT DES R√âSULTATS EN CSV (DATASET R√âEL)...\n");
    
    char csv_filename_real[256];
    time_t now_real = time(NULL);
    struct tm *tm_info_real = localtime(&now_real);
    strftime(csv_filename_real, sizeof(csv_filename_real), "results_exhaustif_heart_attack_%Y%m%d_%H%M%S.csv", tm_info_real);
    
    FILE *csv_file_real = fopen(csv_filename_real, "w");
    if (csv_file_real) {
        // En-t√™te CSV avec m√©tadonn√©es du dataset
        fprintf(csv_file_real, "# NEUROPLAST-ANN - Test Exhaustif Dataset Heart Attack\n");
        fprintf(csv_file_real, "# Dataset: %s (%zu √©chantillons)\n", config_path, dataset->num_samples);
        fprintf(csv_file_real, "# Architecture: Input(%zu)‚Üí256‚Üí128‚ÜíOutput(%zu)\n", dataset->input_cols, dataset->output_cols);
        fprintf(csv_file_real, "# Total combinaisons: %d\n", total_combinations);
        fprintf(csv_file_real, "# Essais par combinaison: 3\n");
        fprintf(csv_file_real, "# √âpoques max: %d\n", max_epochs);
        fprintf(csv_file_real, "# M√©trique principale: F1-Score\n");
        fprintf(csv_file_real, "#\n");
        fprintf(csv_file_real, "Rang,Methode,Optimiseur,Activation,Combinaison_Complete,F1_Score_Moyen_Pct,Meilleur_F1_Pct,Convergence_Count,Total_Trials,Taux_Convergence_Pct\n");
        
        // Donn√©es
        for (int i = 0; i < result_count; i++) {
            fprintf(csv_file_real, "%d,%s,%s,%s,%s,%.2f,%.2f,%d,%d,%.2f\n",
                   i + 1,
                   results[i].method,
                   results[i].optimizer,
                   results[i].activation,
                   results[i].full_name,
                   results[i].avg_f1_score * 100,
                   results[i].best_f1_score * 100,
                   results[i].convergence_count,
                   results[i].total_trials,
                   results[i].convergence_rate * 100);
        }
        
        fclose(csv_file_real);
        printf("‚úÖ R√©sultats export√©s vers : %s\n", csv_filename_real);
        printf("üìä %d combinaisons sauvegard√©es dans le fichier CSV\n", result_count);
        printf("üè• M√©trique principale : F1-Score (adapt√© aux donn√©es m√©dicales)\n");
    } else {
        printf("‚ùå Erreur lors de la cr√©ation du fichier CSV\n");
    }

    printf("\nüèÜ Test exhaustif avec dataset r√©el termin√© avec succ√®s !\n");
    printf("üéØ Utilisez les r√©sultats ci-dessus pour optimiser vos configurations\n");
    printf("üíæ R√©sultats d√©taill√©s disponibles dans : %s\n", csv_filename_real);
    
    return 0;
}

// Test complet de tous les ensembles avec comparaison (dataset XOR simple)
int test_all() {
    printf("üöÄ TEST EXHAUSTIF DE TOUTES LES COMBINAISONS\n");
    printf("===========================================\n\n");
    
    // D√©finir toutes les combinaisons comme dans la commande compl√®te
    const char *neuroplast_methods[] = {"standard", "adaptive", "advanced", "bayesian", "progressive", "swarm", "propagation"};
    const char *optimizers[] = {"adamw", "adam", "sgd", "rmsprop", "lion", "adabelief", "radam", "adamax", "nadam"};
    const char *activations[] = {"neuroplast", "relu", "leaky_relu", "gelu", "sigmoid", "elu", "mish", "swish", "prelu"};
    
    int num_methods = 7;
    int num_optimizers = 9;
    int num_activations = 9;
    int total_combinations = num_methods * num_optimizers * num_activations;
    
    printf("üéØ TEST EXHAUSTIF COMPLET :\n");
    printf("   üìä %d m√©thodes neuroplast\n", num_methods);
    printf("   ‚ö° %d optimiseurs\n", num_optimizers); 
    printf("   üéØ %d fonctions d'activation\n", num_activations);
    printf("   üöÄ %d combinaisons TOTALES\n", total_combinations);
    printf("   üîÑ 3 essais par combinaison\n");
    printf("   üìà 50 √©poques max par essai\n\n");
    
    printf("‚è±Ô∏è Dur√©e estim√©e : 30-45 minutes (mode exhaustif)\n");
    printf("üìä Architecture : Input(2)‚Üí256‚Üí128‚ÜíOutput(1)\n");
    printf("üéØ Dataset : XOR (convergence rapide)\n\n");
    
    // Variables pour collecter les r√©sultats de TOUTES les combinaisons
    typedef struct {
        char method[32];
        char optimizer[32];
        char activation[32];
        char full_name[128];
        float avg_score;
        float best_score;
        int convergence_count;
        int total_trials;
        float convergence_rate;
    } CombinationResult;
    
    CombinationResult *results = malloc(total_combinations * sizeof(CombinationResult));
    if (!results) {
        printf("‚ùå Erreur allocation m√©moire pour %d combinaisons\n", total_combinations);
        return 1;
    }
    
    int result_count = 0;
    int combination_count = 0;
    
    printf("üöÄ D√âMARRAGE DU TEST EXHAUSTIF...\n\n");
    
    // Afficher l'en-t√™te avec l'espace r√©serv√© pour les barres de progression
    progress_show_header(
        "Test exhaustif toutes combinaisons - 3 essais, 50 √©poques", 
        total_combinations,
        3,  // 3 essais par combinaison
        50  // 50 √©poques max par essai
    );
    
    // Initialisation du syst√®me de barres de progression APR√àS l'en-t√™te
    progress_global_init_with_offset(15);  // Offset ajust√© pour le nouvel en-t√™te
    colored_output_set_progress_mode(1);
    colored_output_set_safe_line(20);  // Messages en dessous des barres
    
    // Cr√©er les barres de progression hi√©rarchiques
    int general_bar = progress_global_add(PROGRESS_GENERAL, "Test Exhaustif Complet", total_combinations, 40);
    int trials_bar = progress_global_add(PROGRESS_TRIALS, "Essais par Combinaison", 3, 25);
    int epochs_bar = progress_global_add(PROGRESS_EPOCHS, "Epoques par Essai", 50, 20);
    
    print_info_safe("üéØ Syst√®me de progression initialis√© pour test exhaustif");
    
    // BOUCLE TRIPLE : TOUTES LES COMBINAISONS
    for (int m = 0; m < num_methods; m++) {
        for (int o = 0; o < num_optimizers; o++) {
            for (int a = 0; a < num_activations; a++) {
                combination_count++;
                
                char combo_info[256];
                snprintf(combo_info, sizeof(combo_info), 
                        "üß™ Combinaison %d/%d : %s + %s + %s", 
                        combination_count, total_combinations,
                        neuroplast_methods[m], optimizers[o], activations[a]);
                print_info_safe(combo_info);
                
                // Variables pour moyenner sur plusieurs essais
                float total_score = 0.0f;
                float best_score_combo = 0.0f;
                int convergence_count = 0;
                int trials = 3; // 3 essais par combinaison
                
                // R√©initialiser la barre des essais pour cette combinaison
                progress_global_update(trials_bar, 0, 0.0f, 0.0f, 0.0f);
                
                for (int trial = 0; trial < trials; trial++) {
                    char trial_info[128];
                    snprintf(trial_info, sizeof(trial_info), "Essai %d/%d", trial + 1, trials);
                    print_info_safe(trial_info);
                    
                    size_t layer_sizes[] = {2, 256, 128, 1};
                    const char *test_activations[] = {activations[a], activations[a], "sigmoid"};
                    NeuralNetwork *network = network_create_simple(4, layer_sizes, test_activations);
                    
                    if (!network) {
                        print_info_safe("‚ùå Erreur cr√©ation r√©seau");
                        continue;
                    }
                    
                    // Learning rate adaptatif selon l'optimiseur
                    float lr = 0.001f;
                    if (strcmp(optimizers[o], "sgd") == 0) lr = 0.01f;
                    if (strcmp(optimizers[o], "lion") == 0) lr = 0.0001f;
                    if (strcmp(optimizers[o], "adamw") == 0) lr = 0.002f;
                    
                    // Affichage de la configuration compl√®te (dataset XOR)
                    char config_info[256];
                    snprintf(config_info, sizeof(config_info),
                            "‚ö° Optimiseur: %s | LR: %.4f | M√©thode: %s | Dataset: XOR",
                            optimizers[o], lr, neuroplast_methods[m]);
                    print_info_safe(config_info);
                    
                    float trial_best_score = 0.0f;
                    int trial_convergence = 0;
                    float current_loss = 1.0f;
                    
                    // R√©initialiser la barre des √©poques pour cet essai
                    progress_global_update(epochs_bar, 0, 0.0f, 0.0f, 0.0f);
                    
                    // Entra√Ænement avec barres de progression
                    for (int epoch = 0; epoch < 50; epoch++) {
                        float inputs[4][2] = {{0,0}, {0,1}, {1,0}, {1,1}};
                        float targets[4] = {0, 1, 1, 0};
                        float epoch_loss = 0.0f;
                        
                        for (int sample = 0; sample < 4; sample++) {
                            network_forward_simple(network, inputs[sample]);
                            float target_array[] = {targets[sample]};
                            network_backward_simple(network, inputs[sample], target_array, lr);
                            
                            // Calculer loss pour affichage
                            float *output = network_output_simple(network);
                            float error = output[0] - targets[sample];
                            epoch_loss += error * error;
                        }
                        
                        current_loss = epoch_loss / 4.0f;
                        
                        // Test et mise √† jour des barres de progression (optimis√© toutes les 5 √©poques)
                        if (epoch % 5 == 0 || epoch == 49) {
                            int correct = 0;
                            for (int sample = 0; sample < 4; sample++) {
                                network_forward_simple(network, inputs[sample]);
                                float *output = network_output_simple(network);
                                if (fabs((output[0] > 0.5f ? 1.0f : 0.0f) - targets[sample]) < 0.1f) correct++;
                            }
                            float score = (float)correct / 4.0f;
                            if (score > trial_best_score) trial_best_score = score;
                            if (score >= 1.0f && !trial_convergence) trial_convergence = 1;
                            
                            // Mettre √† jour la barre des √©poques avec m√©triques toutes les 5 √©poques
                            float learning_rate_display = (epoch < 25) ? lr : lr * 0.5f; // LR decay simul√©
                            progress_global_update(epochs_bar, epoch + 1, current_loss, score, learning_rate_display);
                        }
                        
                        // Suppression de la pause pour acc√©l√©ration (optimisation performance)
                    }
                    
                    total_score += trial_best_score;
                    if (trial_best_score > best_score_combo) best_score_combo = trial_best_score;
                    if (trial_convergence) convergence_count++;
                    
                    // Mettre √† jour la barre des essais
                    float trial_loss = (trial_best_score > 0) ? (1.0f - trial_best_score) : current_loss;
                    progress_global_update(trials_bar, trial + 1, trial_loss, trial_best_score, lr);
                    
                    network_free_simple(network);
                }
                
                // Stocker le r√©sultat de cette combinaison
                strcpy(results[result_count].method, neuroplast_methods[m]);
                strcpy(results[result_count].optimizer, optimizers[o]);
                strcpy(results[result_count].activation, activations[a]);
                snprintf(results[result_count].full_name, sizeof(results[result_count].full_name),
                        "%s+%s+%s", neuroplast_methods[m], optimizers[o], activations[a]);
                
                results[result_count].avg_score = total_score / trials;
                results[result_count].best_score = best_score_combo;
                results[result_count].convergence_count = convergence_count;
                results[result_count].total_trials = trials;
                results[result_count].convergence_rate = (float)convergence_count / trials;
                
                // Affichage s√©curis√© des r√©sultats de cette combinaison
                char combo_result[256];
                snprintf(combo_result, sizeof(combo_result),
                        "üìä Avg: %.1f%% | Best: %.1f%% | Convergence: %d/%d (%.0f%%)",
                        results[result_count].avg_score * 100,
                        results[result_count].best_score * 100,
                        convergence_count, trials,
                        results[result_count].convergence_rate * 100);
                print_info_safe(combo_result);
                
                result_count++;
                
                // Mettre √† jour la barre de progression g√©n√©rale
                float general_progress = (float)combination_count / total_combinations;
                float avg_loss = (results[result_count-1].avg_score > 0) ? (1.0f - results[result_count-1].avg_score) : 1.0f;
                float current_accuracy = results[result_count-1].avg_score;
                progress_global_update(general_bar, combination_count, avg_loss, current_accuracy, general_progress);
            }
        }
    }
    
    // ANALYSE DES R√âSULTATS EXHAUSTIFS
    printf("\nüî∏ ANALYSE DES R√âSULTATS EXHAUSTIFS\n");
    printf("===================================\n\n");
    
    printf("üèÜ ANALYSE COMPL√àTE DE %d COMBINAISONS\n", total_combinations);
    printf("====================================\n\n");
    
    // Trier par score moyen (bubble sort)
    for (int i = 0; i < result_count - 1; i++) {
        for (int j = 0; j < result_count - i - 1; j++) {
            if (results[j].avg_score < results[j + 1].avg_score) {
                CombinationResult temp = results[j];
                results[j] = results[j + 1];
                results[j + 1] = temp;
            }
        }
    }
    
    // TOP 10 des meilleures combinaisons
    printf("ü•á TOP 10 DES MEILLEURES COMBINAISONS :\n");
    printf("Rang | Combinaison                          | Avg Score | Best Score | Conv Rate\n");
    printf("-----|--------------------------------------|-----------|------------|----------\n");
    
    int top_display = (result_count < 10) ? result_count : 10;
    for (int i = 0; i < top_display; i++) {
        printf("%4d | %-36s | %8.1f%% | %9.1f%% | %8.0f%%\n", 
               i + 1, 
               results[i].full_name,
               results[i].avg_score * 100,
               results[i].best_score * 100,
               results[i].convergence_rate * 100);
    }
    
    // Statistiques par m√©thode neuroplast
    printf("\nüìä PERFORMANCES MOYENNES PAR M√âTHODE NEUROPLAST :\n");
    for (int m = 0; m < num_methods; m++) {
        float total_score = 0.0f;
        int count = 0;
        
        for (int i = 0; i < result_count; i++) {
            if (strcmp(results[i].method, neuroplast_methods[m]) == 0) {
                total_score += results[i].avg_score;
                count++;
            }
        }
        
        if (count > 0) {
            printf("   %-12s : %.1f%% (sur %d combinaisons)\n", 
                   neuroplast_methods[m], (total_score / count) * 100, count);
        }
    }
    
    // Statistiques par optimiseur
    printf("\n‚ö° PERFORMANCES MOYENNES PAR OPTIMISEUR :\n");
    for (int o = 0; o < num_optimizers; o++) {
        float total_score = 0.0f;
        int count = 0;
        
        for (int i = 0; i < result_count; i++) {
            if (strcmp(results[i].optimizer, optimizers[o]) == 0) {
                total_score += results[i].avg_score;
                count++;
            }
        }
        
        if (count > 0) {
            printf("   %-12s : %.1f%% (sur %d combinaisons)\n", 
                   optimizers[o], (total_score / count) * 100, count);
        }
    }
    
    // Statistiques par activation
    printf("\nüéØ PERFORMANCES MOYENNES PAR ACTIVATION :\n");
    for (int a = 0; a < num_activations; a++) {
        float total_score = 0.0f;
        int count = 0;
        
        for (int i = 0; i < result_count; i++) {
            if (strcmp(results[i].activation, activations[a]) == 0) {
                total_score += results[i].avg_score;
                count++;
            }
        }
        
        if (count > 0) {
            printf("   %-12s : %.1f%% (sur %d combinaisons)\n", 
                   activations[a], (total_score / count) * 100, count);
        }
    }
    
    // Combinaisons avec convergence parfaite
    printf("\n‚úÖ COMBINAISONS AVEC 100%% DE CONVERGENCE :\n");
    int perfect_count = 0;
    for (int i = 0; i < result_count; i++) {
        if (results[i].convergence_rate >= 1.0f && results[i].avg_score >= 0.9f) {
            printf("   %s (%.1f%% avg)\n", results[i].full_name, results[i].avg_score * 100);
            perfect_count++;
        }
    }
    if (perfect_count == 0) {
        printf("   Aucune combinaison avec convergence parfaite trouv√©e.\n");
    }
    
    // Recommandations finales
    printf("\nüéä RECOMMANDATIONS FINALES EXHAUSTIVES :\n");
    if (result_count > 0) {
        printf("ü•á Meilleure combinaison : %s\n", results[0].full_name);
        printf("   üìä Score moyen : %.1f%%\n", results[0].avg_score * 100);
        printf("   üèÜ Meilleur score : %.1f%%\n", results[0].best_score * 100);
        printf("   ‚úÖ Taux de convergence : %.0f%%\n", results[0].convergence_rate * 100);
        printf("üéØ Architecture test√©e : Input(2)‚Üí256‚Üí128‚ÜíOutput(1)\n");
        printf("üìà Total combinaisons test√©es : %d sur %d possibles\n", result_count, total_combinations);
        printf("‚è±Ô∏è √âquivalent √† la commande compl√®te avec --test-all !\n");
    }
    
    // Finaliser les barres de progression
    progress_global_finish(general_bar);
    progress_global_finish(trials_bar);
    progress_global_finish(epochs_bar);
    
    // D√©sactiver le mode progression s√©curis√© pour les messages finaux
    colored_output_set_progress_mode(0);
    
    // Lib√©ration m√©moire
    free(results);
    
    // Nettoyer le syst√®me de progression
    progress_global_cleanup();
    
    // EXPORT CSV DES R√âSULTATS EXHAUSTIFS
    printf("\nüìä EXPORT DES R√âSULTATS EN CSV...\n");
    
    char csv_filename[256];
    time_t now = time(NULL);
    struct tm *tm_info = localtime(&now);
    strftime(csv_filename, sizeof(csv_filename), "results_exhaustif_xor_%Y%m%d_%H%M%S.csv", tm_info);
    
    FILE *csv_file = fopen(csv_filename, "w");
    if (csv_file) {
        // En-t√™te CSV
        fprintf(csv_file, "Rang,Methode,Optimiseur,Activation,Combinaison_Complete,Score_Moyen_Pct,Meilleur_Score_Pct,Convergence_Count,Total_Trials,Taux_Convergence_Pct\n");
        
        // Donn√©es
        for (int i = 0; i < result_count; i++) {
            fprintf(csv_file, "%d,%s,%s,%s,%s,%.2f,%.2f,%d,%d,%.2f\n",
                   i + 1,
                   results[i].method,
                   results[i].optimizer,
                   results[i].activation,
                   results[i].full_name,
                   results[i].avg_score * 100,
                   results[i].best_score * 100,
                   results[i].convergence_count,
                   results[i].total_trials,
                   results[i].convergence_rate * 100);
        }
        
        fclose(csv_file);
        printf("‚úÖ R√©sultats export√©s vers : %s\n", csv_filename);
        printf("üìä %d combinaisons sauvegard√©es dans le fichier CSV\n", result_count);
    } else {
        printf("‚ùå Erreur lors de la cr√©ation du fichier CSV\n");
    }

    printf("\nüèÜ Test exhaustif termin√© avec succ√®s !\n");
    printf("üéØ Utilisez les r√©sultats ci-dessus pour optimiser vos configurations\n");
    printf("üíæ R√©sultats d√©taill√©s disponibles dans : %s\n", csv_filename);
    
    return 0;
}
// Benchmark complet avec m√©triques d√©taill√©es
int test_benchmark_full() {
    printf("üìä BENCHMARK COMPLET AVEC M√âTRIQUES D√âTAILL√âES\n");
    printf("===============================================\n\n");
    
    printf("üéØ Ce test compare les performances de toutes les configurations\n");
    printf("üìà M√©triques : Vitesse, Accuracy, Stabilit√©, Efficacit√© m√©moire\n");
    printf("‚è±Ô∏è Dur√©e estim√©e : 2-3 minutes\n\n");
    
    // Test rapide de quelques configurations cl√©s
    const char *configs[][3] = {
        {"neuroplast", "adamw", "advanced"},
        {"gelu", "adam", "standard"},
        {"relu", "sgd", "adaptive"},
        {"mish", "radam", "bayesian"}
    };
    int num_configs = 4;
    
    for (int i = 0; i < num_configs; i++) {
        printf("üß™ Benchmark %d/%d : %s + %s + %s\n", 
               i+1, num_configs, configs[i][0], configs[i][1], configs[i][2]);
        
        // Mesure du temps de cr√©ation
        clock_t start_time = clock();
        
        size_t layer_sizes[] = {2, 256, 128, 1};
        const char *activations[] = {configs[i][0], configs[i][0], "sigmoid"};
        NeuralNetwork *network = network_create_simple(4, layer_sizes, activations);
        
        clock_t creation_time = clock() - start_time;
        
        if (!network) {
            printf("   ‚ùå Erreur cr√©ation r√©seau\n");
            continue;
        }
        
        // Mesure du temps d'entra√Ænement
        start_time = clock();
        
        float best_accuracy = 0.0f;
        int stable_epochs = 0;
        
        for (int epoch = 0; epoch < 100; epoch++) {
            float inputs[4][2] = {{0,0}, {0,1}, {1,0}, {1,1}};
            float targets[4] = {0, 1, 1, 0};
            
            for (int sample = 0; sample < 4; sample++) {
                network_forward_simple(network, inputs[sample]);
                float target_array[] = {targets[sample]};
                network_backward_simple(network, inputs[sample], target_array, 0.001f);
            }
            
            // Test accuracy
            if (epoch % 10 == 0) {
                int correct = 0;
                for (int sample = 0; sample < 4; sample++) {
                    network_forward_simple(network, inputs[sample]);
                    float *output = network_output_simple(network);
                    float prediction = output[0] > 0.5f ? 1.0f : 0.0f;
                    if (fabs(prediction - targets[sample]) < 0.1f) correct++;
                }
                float accuracy = (float)correct / 4.0f;
                
                if (fabs(accuracy - best_accuracy) < 0.1f) {
                    stable_epochs++;
                } else {
                    stable_epochs = 0;
                }
                best_accuracy = fmax(best_accuracy, accuracy);
            }
        }
        
        clock_t training_time = clock() - start_time;
        
        printf("   ‚è±Ô∏è Temps cr√©ation: %ld ms\n", creation_time * 1000 / CLOCKS_PER_SEC);
        printf("   ‚ö° Temps entra√Ænement: %ld ms\n", training_time * 1000 / CLOCKS_PER_SEC);
        printf("   üìä Accuracy finale: %.1f%%\n", best_accuracy * 100);
        printf("   üéØ Stabilit√©: %d √©poques stables\n", stable_epochs);
        printf("   üíæ Efficacit√©: %s\n", 
               (creation_time + training_time) < CLOCKS_PER_SEC ? "Excellente" : "Bonne");
        printf("\n");
        
        network_free_simple(network);
    }
    
    printf("üèÜ Benchmark complet termin√© !\n");
    return 0;
}

int main(int argc, char *argv[]) {
    // Initialiser le g√©n√©rateur de nombres al√©atoires
    srand(time(NULL));
    
    print_banner();
    
    // V√©rifier si c'est un mode de test
    RunMode mode = get_run_mode(argc, argv);
    
    if (mode != MODE_DEFAULT) {
        printf("üß™ MODE TEST ACTIV√â\n\n");
        
        switch (mode) {
            case MODE_TEST_HEART_DISEASE:
                printf("ü´Ä Test sp√©cialis√© Heart Disease\n");
                // TODO: Impl√©menter le test heart disease
                break;
            case MODE_TEST_ENHANCED:
                printf("‚ö° Test Enhanced Network\n");
                // TODO: Impl√©menter le test enhanced
                break;
            case MODE_TEST_ROBUST:
                printf("üõ°Ô∏è Test Robust Network\n");
                // TODO: Impl√©menter le test robust
                break;
            case MODE_TEST_OPTIMIZED_METRICS:
                printf("üìä Test Optimized Metrics\n");
                // TODO: Impl√©menter le test optimized metrics
                break;
            case MODE_TEST_ALL_ACTIVATIONS:
                return test_all_activations();
            case MODE_TEST_ALL_OPTIMIZERS:
                return test_all_optimizers();
            case MODE_TEST_NEUROPLAST_METHODS:
                return test_neuroplast_methods();
            case MODE_TEST_COMPLETE_COMBINATIONS:
                return test_complete_combinations();
            case MODE_TEST_BENCHMARK_FULL:
                return test_benchmark_full();
            case MODE_TEST_ALL:
                return test_all();
            default:
                break;
        }
        
        printf("‚úÖ Test termin√© avec succ√®s\n");
        return EXIT_SUCCESS;
    }
    
    // Ajouter un espace apr√®s le banner pour √©viter la superposition
    printf("\n");
    
    // Parsing des arguments AVANT tout
    CommandLineArgs args = {0};
    if (!args_parse(argc, argv, &args)) {
        return EXIT_FAILURE;
    }

    // Chargement de la configuration rich avec fallback
    RichConfig cfg = {0};
    bool config_loaded = parse_yaml_rich_config(args.config_path, &cfg);
    
    // Si en mode compare_all_methods avec tous les param√®tres CLI, utiliser des valeurs par d√©faut si pas de config
    if (args.mode == MODE_COMPARE_ALL_METHODS && args.has_neuroplast_methods && args.has_optimizers && args.has_activations) {
        if (!config_loaded) {
            printf("‚ö†Ô∏è Configuration YAML non trouv√©e, utilisation des valeurs par d√©faut\n");
            // Valeurs par d√©faut pour le mode compare_all_methods
            cfg.max_epochs = 100;
            cfg.batch_size = 32;
            cfg.learning_rate = 0.001f;
            strcpy(cfg.dataset, "config/heart_attack.yml"); // Dataset par d√©faut
        }
    } else {
        // Pour les autres modes, la config YAML est obligatoire
        if (!config_loaded) {
            printf("Erreur: impossible de charger la configuration %s\n", args.config_path);
            return EXIT_FAILURE;
        }
    }
    
    // Fusion avec les arguments CLI
    args_merge_config(&args, &cfg);
    
    // V√©rifier si on est en mode compare_all_methods avec tous les param√®tres
    if (args.mode == MODE_COMPARE_ALL_METHODS && args.has_neuroplast_methods && args.has_optimizers && args.has_activations) {
        printf("üöÄ MODE COMPARE_ALL_METHODS D√âTECT√â - Lancement du test exhaustif avec barres de progression\n\n");
        
        // Utiliser les param√®tres CLI au lieu de la config YAML
        const char **neuroplast_methods = (const char**)malloc(args.num_neuroplast_methods * sizeof(char*));
        const char **optimizers = (const char**)malloc(args.num_optimizers * sizeof(char*));
        const char **activations = (const char**)malloc(args.num_activations * sizeof(char*));
        
        for (int i = 0; i < args.num_neuroplast_methods; i++) {
            neuroplast_methods[i] = args.neuroplast_methods[i];
        }
        for (int i = 0; i < args.num_optimizers; i++) {
            optimizers[i] = args.optimizers[i];
        }
        for (int i = 0; i < args.num_activations; i++) {
            activations[i] = args.activations[i];
        }
        
        int total_combinations = args.num_neuroplast_methods * args.num_optimizers * args.num_activations;
        
        printf("üéØ CONFIGURATION D√âTECT√âE :\n");
        printf("   üìä %d m√©thodes neuroplast\n", args.num_neuroplast_methods);
        printf("   ‚ö° %d optimiseurs\n", args.num_optimizers); 
        printf("   üéØ %d fonctions d'activation\n", args.num_activations);
        printf("   üöÄ %d combinaisons TOTALES\n", total_combinations);
        printf("   üîÑ 3 essais par combinaison\n");
        printf("   üìà 100 √©poques max par essai\n\n");
        
        printf("‚è±Ô∏è Dur√©e estim√©e : 30-45 minutes (mode exhaustif)\n");
        printf("üìä Architecture : Input‚Üí256‚Üí128‚ÜíOutput\n");
        printf("üéØ Dataset : %s\n\n", args.config_path);
        
        // Lancer le test exhaustif avec la logique de test_all() mais avec le vrai dataset
        // Utiliser le fichier de dataset sp√©cifi√©, ou le dataset par d√©faut si pas de config
        const char *dataset_path = config_loaded ? args.config_path : "config/heart_attack.yml";
        int result = test_all_with_real_dataset(neuroplast_methods, args.num_neuroplast_methods,
                                               optimizers, args.num_optimizers,
                                               activations, args.num_activations,
                                               dataset_path, cfg.max_epochs);
        
        free((char**)neuroplast_methods);
        free((char**)optimizers);  
        free((char**)activations);
        
        return result;
    }
    
    // Calcul du nombre total de combinaisons APR√àS le chargement de la config (mode normal)
    int total_combinations = cfg.num_neuroplast_methods * cfg.num_optimizers * cfg.num_activations;
    
    // Affichage de l'en-t√™te descriptif du test
    progress_show_header(
        "Test exhaustif toutes combinaisons - 5 essais, 100 √©poques",
        total_combinations,
        5,  // num_trials par d√©faut
        cfg.max_epochs  // Utiliser la valeur de la config
    );
    
    // Initialisation du syst√®me de barres de progression APR√àS l'en-t√™te
    progress_global_init_with_offset(15);  // Ajust√© pour le nouvel en-t√™te
    
    // Activer le mode progression s√©curis√© pour √©viter les superpositions
    colored_output_set_progress_mode(1);
    colored_output_set_safe_line(20);  // Messages en dessous des barres de progression
    
    // Chargement du dataset
    Dataset *dataset = load_dataset_from_yaml(args.config_path);
    if (!dataset) {
        printf("Erreur: impossible de charger le dataset\n");
        progress_global_cleanup();
        return EXIT_FAILURE;
    }
    
    printf("Dataset charg√©: %zu samples, %zu inputs, %zu outputs\n", 
           dataset->num_samples, dataset->input_cols, dataset->output_cols);

    // Division train/test
    Dataset *train_set = NULL, *test_set = NULL;
    split_dataset(dataset, 0.8f, &train_set, &test_set);
    
    printf("Division train/test - Train: %zu, Test: %zu\n", 
           train_set->num_samples, test_set->num_samples);

    // BOUCLE CORRIG√âE POUR TESTER TOUTES LES COMBINAISONS DE LA CONFIG
    int total_experiments = cfg.num_neuroplast_methods * cfg.num_optimizers * cfg.num_activations;
    
    // Cr√©er le syst√®me de barres de progression hi√©rarchique
    int general_bar = progress_global_add(PROGRESS_GENERAL, "Test Toutes Combinaisons", total_experiments, 50);
    int trials_bar = progress_global_add(PROGRESS_TRIALS, "Essais par Combinaison", 5, 30);
    int epochs_bar = progress_global_add(PROGRESS_EPOCHS, "√âpoques par Essai", cfg.max_epochs, 25);
    
    int experiment_count = 0;
    
    printf("üöÄ D√âMARRAGE DU TEST DE TOUTES LES COMBINAISONS...\n\n");
    
    // BOUCLE TRIPLE CORRIG√âE : TOUTES LES COMBINAISONS DE LA CONFIG
    for (int m = 0; m < cfg.num_neuroplast_methods; m++) {
        for (int o = 0; o < cfg.num_optimizers; o++) {
            for (int a = 0; a < cfg.num_activations; a++) {
                experiment_count++;
                
                // AFFICHAGE CLAIR DE LA COMBINAISON ACTUELLE
                printf("\nüéØ ======== COMBINAISON %d/%d ========\n", experiment_count, total_experiments);
                printf("   üìä M√©thode neuroplast: %s\n", cfg.neuroplast_methods[m].name);
                printf("   ‚ö° Optimiseur: %s\n", cfg.optimizers[o].name);
                printf("   üéØ Activation: %s\n", cfg.activations[a].name);
                printf("   üîÑ Tests: 5 essais √ó %d √©poques\n", cfg.max_epochs);
                printf("=======================================\n");
                
                char experiment_info[256];
                snprintf(experiment_info, sizeof(experiment_info), 
                        "üß™ Combinaison %d/%d : %s + %s + %s", 
                        experiment_count, total_experiments,
                        cfg.neuroplast_methods[m].name, cfg.optimizers[o].name, cfg.activations[a].name);
                print_info_safe(experiment_info);

                // Variables pour moyenner sur plusieurs essais
                float total_score = 0.0f;
                float best_score_combo = 0.0f;
                int convergence_count = 0;
                int trials = 5; // 5 essais par combinaison
                
                // R√©initialiser la barre des essais pour cette combinaison
                progress_global_update(trials_bar, 0, 0.0f, 0.0f, 0.0f);
                
                for (int trial = 0; trial < trials; trial++) {
                    char trial_info[128];
                    snprintf(trial_info, sizeof(trial_info), "Essai %d/%d", trial + 1, trials);
                    print_info_safe(trial_info);
                    
                    // Architecture am√©lior√©e avec plus de neurones par couche
                    size_t layer_sizes[] = {dataset->input_cols, 256, 128, dataset->output_cols}; // Input ‚Üí 256 ‚Üí 128 ‚Üí Output
                    
                    // Utiliser l'activation sp√©cifi√©e dans la configuration
                    const char *test_activations[] = {cfg.activations[a].name, cfg.activations[a].name, "sigmoid"};
                    
                    // Cr√©ation du r√©seau am√©lior√©
                    NeuralNetwork *network = network_create_simple(4, layer_sizes, test_activations);
                    if (!network) {
                        print_info_safe("‚ùå Erreur cr√©ation r√©seau");
                        continue;
                    }
                    
                    // Learning rate adaptatif selon l'optimiseur
                    float lr = 0.001f;
                    if (strcmp(cfg.optimizers[o].name, "sgd") == 0) lr = 0.01f;
                    if (strcmp(cfg.optimizers[o].name, "lion") == 0) lr = 0.0001f;
                    if (strcmp(cfg.optimizers[o].name, "adamw") == 0) lr = 0.002f;
                    
                    // Affichage de la configuration compl√®te
                    char config_info[256];
                    snprintf(config_info, sizeof(config_info),
                            "‚ö° Optimiseur: %s | LR: %.4f | M√©thode: %s",
                            cfg.optimizers[o].name, lr, cfg.neuroplast_methods[m].name);
                    
                    printf("\nüîß CONFIGURATION ACTUELLE:\n");
                    printf("   %s\n\n", config_info);
                    print_info_safe(config_info);
                    
                    float trial_best_score = 0.0f;
                    int trial_convergence = 0;
                    
                    // R√©initialiser la barre des √©poques pour cet essai
                    progress_global_update(epochs_bar, 0, 0.0f, 0.0f, 0.0f);

                    // ENTRA√éNEMENT POUR CET ESSAI
                    int batch_size = 32;
                    float current_loss = 1.0f;
                    
                    // Boucle d'entra√Ænement pour cet essai
                    for (int epoch = 0; epoch < cfg.max_epochs; epoch++) {
                        // Entra√Ænement par mini-batches
                        current_loss = 0.0f;
                        for (size_t batch_start = 0; batch_start < train_set->num_samples; batch_start += batch_size) {
                            size_t batch_end = (batch_start + batch_size < train_set->num_samples) ? 
                                              batch_start + batch_size : train_set->num_samples;
                            
                            for (size_t i = batch_start; i < batch_end; i++) {
                                network_forward_simple(network, train_set->inputs[i]);
                                network_backward_simple(network, train_set->inputs[i], train_set->outputs[i], lr);
                                
                                // Calculer loss pour affichage
                                float *output = network_output_simple(network);
                                float error = output[0] - train_set->outputs[i][0];
                                current_loss += error * error;
                            }
                        }
                        
                        current_loss = current_loss / train_set->num_samples;
                        
                        // Test et mise √† jour des barres de progression (optimis√© toutes les 10 √©poques)
                        if (epoch % 10 == 0 || epoch == cfg.max_epochs - 1) {
                            AllMetrics test_metrics = compute_all_metrics(network, test_set);
                            float score = test_metrics.f1_score; // Utiliser F1-score comme m√©trique principale
                            
                            if (score > trial_best_score) trial_best_score = score;
                            if (score >= 0.8f && !trial_convergence) trial_convergence = 1; // Convergence √† 80% F1
                            
                            // Mettre √† jour la barre des √©poques avec m√©triques toutes les 10 √©poques
                            float learning_rate_display = (epoch < cfg.max_epochs/2) ? lr : lr * 0.5f; // LR decay simul√©
                            progress_global_update(epochs_bar, epoch + 1, current_loss, score, learning_rate_display);
                        }
                        
                        // Early stopping pour √©viter l'overfitting
                        if (trial_convergence && epoch > cfg.max_epochs / 3) {
                            print_info_safe("‚úÖ Convergence pr√©coce d√©tect√©e");
                            break;
                        }
                    }
                    
                    total_score += trial_best_score;
                    if (trial_best_score > best_score_combo) best_score_combo = trial_best_score;
                    if (trial_convergence) convergence_count++;
                    
                    // Mettre √† jour la barre des essais
                    float trial_loss = (trial_best_score > 0) ? (1.0f - trial_best_score) : current_loss;
                    progress_global_update(trials_bar, trial + 1, trial_loss, trial_best_score, lr);
                    
                    network_free_simple(network);
                }
                
                // Affichage des r√©sultats de cette combinaison
                char combo_result[256];
                snprintf(combo_result, sizeof(combo_result),
                        "üìä Avg F1: %.1f%% | Best F1: %.1f%% | Convergence: %d/%d (%.0f%%)",
                        (total_score / trials) * 100,
                        best_score_combo * 100,
                        convergence_count, trials,
                        ((float)convergence_count / trials) * 100);
                print_info_safe(combo_result);
                
                // Mettre √† jour la barre de progression g√©n√©rale
                float general_progress = (float)experiment_count / total_experiments;
                float avg_loss = (total_score > 0) ? (1.0f - total_score / trials) : 1.0f;
                float current_accuracy = total_score / trials;
                progress_global_update(general_bar, experiment_count, avg_loss, current_accuracy, general_progress);
            }
        }
    }
    
    // ANALYSE DES R√âSULTATS DE TOUTES LES COMBINAISONS
    printf("\nüî∏ ANALYSE DES R√âSULTATS DE TOUTES LES COMBINAISONS\n");
    printf("===================================================\n\n");
    
    printf("üèÜ ANALYSE COMPL√àTE DE %d COMBINAISONS\n", total_experiments);
    printf("====================================\n\n");
    
    // Finaliser le syst√®me de barres de progression
    progress_global_finish(general_bar);
    progress_global_finish(trials_bar);
    progress_global_finish(epochs_bar);
    
    // D√©sactiver le mode progression s√©curis√© pour les messages finaux
    colored_output_set_progress_mode(0);

    // Nettoyage final
    dataset_free(dataset);
    dataset_free(train_set);
    dataset_free(test_set);
    
    // Nettoyer le syst√®me de progression
    progress_global_cleanup();
    
    printf("\nüèÜ Test de toutes les combinaisons termin√© avec succ√®s !\n");
    printf("üéØ Utilisez les r√©sultats ci-dessus pour optimiser vos configurations\n");
    
    return EXIT_SUCCESS;
}