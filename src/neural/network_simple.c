#include "network.h"
#include "network_simple.h"
#include "activation.h"
#include "../colored_output.h"
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <string.h>

#ifndef M_PI
#define M_PI 3.14159265358979323846
#endif

// NOUVELLES FONCTIONS DE CONFIGURATION

// Fonction pour cr√©er une configuration par d√©faut
NetworkConfig create_default_config() {
    NetworkConfig config;
    config.learning_rate = 0.001f;      // Plus conservateur que la valeur hardcod√©e
    config.momentum = 0.9f;
    config.dropout_rate = 0.25f;
    config.l2_lambda = 0.0005f;
    config.optimal_threshold = 0.15f;
    config.class_weight_ratio = 15.0f;  // 15x pour classe minoritaire
    config.use_momentum = 1;
    config.use_dropout = 1;
    return config;
}

// Fonction pour cr√©er une configuration optimis√©e selon l'optimiseur
NetworkConfig create_config_for_optimizer(const char *optimizer_name) {
    NetworkConfig config = create_default_config();
    
    if (strcmp(optimizer_name, "sgd") == 0) {
        config.learning_rate = 0.01f;      // SGD a besoin d'un LR plus √©lev√©
        config.momentum = 0.9f;
        config.dropout_rate = 0.3f;        // Plus de r√©gularisation pour SGD
    } else if (strcmp(optimizer_name, "adam") == 0) {
        config.learning_rate = 0.001f;     // Adam standard
        config.momentum = 0.9f;            // Beta1 √©quivalent
        config.dropout_rate = 0.2f;        // Moins de dropout avec Adam
    } else if (strcmp(optimizer_name, "adamw") == 0) {
        config.learning_rate = 0.002f;     // AdamW peut √™tre plus agressif
        config.momentum = 0.9f;
        config.l2_lambda = 0.001f;         // Plus de r√©gularisation L2
        config.dropout_rate = 0.15f;       // Moins de dropout avec forte L2
    } else if (strcmp(optimizer_name, "lion") == 0) {
        config.learning_rate = 0.0001f;    // Lion utilise des LR tr√®s bas
        config.momentum = 0.99f;           // Momentum √©lev√© pour Lion
        config.dropout_rate = 0.25f;
    } else if (strcmp(optimizer_name, "radam") == 0) {
        config.learning_rate = 0.0015f;    // RAdam interm√©diaire
        config.momentum = 0.9f;
        config.dropout_rate = 0.2f;
    }
    
    return config;
}

// Structure simplifi√©e et robuste avec am√©liorations anti-overfitting
typedef struct {
    size_t num_layers;
    Layer **layers;
    float learning_rate;
    float momentum;
    float **momentum_weights; // Momentum pour SGD avec momentum
    float *momentum_biases;
    int use_momentum;
    
    // NOUVELLES FONCTIONNALIT√âS
    float class_weights[2];     // Poids pour √©quilibrage des classes [classe_0, classe_1]
    float dropout_rate;         // Taux de dropout
    float l2_lambda;           // Coefficient de r√©gularisation L2
    float optimal_threshold;    // Seuil de d√©cision optimal
    int use_dropout;           // Activer/d√©sactiver dropout
    float *dropout_mask;       // Masque de dropout pour la couche cach√©e
} SimpleNeuralNetwork;

// Initialisation HE pour ReLU et Xavier pour Sigmoid/Tanh (√©prouv√©e)
static void init_weights_simple(Layer *layer, activation_type_t activation) {
    float fan_in = (float)layer->input_size;
    float fan_out = (float)layer->output_size;
    float std;
    
    // Choix de l'initialisation optimale selon l'activation
    switch (activation) {
        case ACTIVATION_RELU:
        case ACTIVATION_LEAKY_RELU:
        case ACTIVATION_PRELU:
            // He initialization pour ReLU et variantes (2x plus de variance)
            std = sqrtf(2.0f / fan_in);
            break;
            
        case ACTIVATION_GELU:
        case ACTIVATION_MISH:
        case ACTIVATION_SWISH:
            // Initialisation l√©g√®rement plus conservative pour activations smooth
            std = sqrtf(1.8f / fan_in);
            break;
            
        case ACTIVATION_ELU:
            // ELU : initialisation interm√©diaire
            std = sqrtf(1.5f / fan_in);
            break;
            
        case ACTIVATION_SIGMOID:
        case ACTIVATION_TANH:
            // Xavier/Glorot pour Sigmoid/Tanh
            std = sqrtf(2.0f / (fan_in + fan_out));
            break;
            
        case ACTIVATION_NEUROPLAST:
            // NeuroPlast : initialisation adaptative plus aggressive
            std = sqrtf(2.5f / fan_in);
            break;
            
        default:
            // Par d√©faut : Xavier modifi√©
            std = sqrtf(2.0f / (fan_in + fan_out));
    }
    
    // Distribution normale avec Box-Muller
    for (size_t i = 0; i < layer->output_size; i++) {
        for (size_t j = 0; j < layer->input_size; j++) {
            static int has_spare = 0;
            static float spare;
            
            if (has_spare) {
                has_spare = 0;
                layer->weights[i][j] = spare * std;
            } else {
                has_spare = 1;
                float u, v, mag;
                do {
                    u = 2.0f * ((float)rand() / RAND_MAX) - 1.0f;
                    v = 2.0f * ((float)rand() / RAND_MAX) - 1.0f;
                    mag = u * u + v * v;
                } while (mag >= 1.0f || mag == 0.0f);
                
                mag = sqrtf(-2.0f * logf(mag) / mag);
                spare = v * mag;
                layer->weights[i][j] = u * mag * std;
            }
        }
        // Biais initialis√©s √† z√©ro pour les couches cach√©es (sauf cas sp√©ciaux)
        if (activation == ACTIVATION_NEUROPLAST) {
            // NeuroPlast : petit biais al√©atoire pour briser la sym√©trie
            layer->biases[i] = ((float)rand() / RAND_MAX - 0.5f) * 0.01f;
        } else {
            layer->biases[i] = 0.0f;
        }
    }
}

// Activation simple et robuste
static float apply_activation(float x, activation_type_t type) {
    switch (type) {
        case ACTIVATION_RELU:
            return fmaxf(0.0f, x);
        case ACTIVATION_SIGMOID:
            // Protection contre overflow
            if (x > 500.0f) return 1.0f;
            if (x < -500.0f) return 0.0f;
            return 1.0f / (1.0f + expf(-x));
        case ACTIVATION_TANH:
            // Protection contre overflow
            if (x > 500.0f) return 1.0f;
            if (x < -500.0f) return -1.0f;
            return tanhf(x);
        case ACTIVATION_LEAKY_RELU:
            return (x > 0) ? x : 0.01f * x;
        case ACTIVATION_LINEAR:
            return x;
        // NOUVELLES ACTIVATIONS AJOUT√âES
        case ACTIVATION_GELU:
            // GELU approximation: 0.5 * x * (1 + tanh(sqrt(2/œÄ) * (x + 0.044715 * x^3)))
            return 0.5f * x * (1.0f + tanhf(0.7978845608f * (x + 0.044715f * x * x * x)));
        case ACTIVATION_MISH:
            // Mish: x * tanh(ln(1 + e^x))
            return x * tanhf(logf(1.0f + expf(fminf(x, 20.0f)))); // Protection overflow
        case ACTIVATION_SWISH:
            // Swish: x * sigmoid(x)
            if (x > 500.0f) return x;
            if (x < -500.0f) return 0.0f;
            return x / (1.0f + expf(-x));
        case ACTIVATION_ELU:
            // ELU: x si x > 0, sinon Œ±(e^x - 1) avec Œ± = 1.0
            return (x > 0) ? x : (expf(x) - 1.0f);
        case ACTIVATION_PRELU:
            // PReLU avec Œ± = 0.01 (peut √™tre fait param√©trable plus tard)
            return (x > 0) ? x : 0.01f * x;
        default:
            // ACTIVATION_NEUROPLAST ou inconnue : utiliser une fonction adaptative
            // NeuroPlast: m√©lange adaptatif ReLU/Tanh selon la valeur
            if (x > 1.0f) return x; // ReLU pour grandes valeurs
            else if (x < -1.0f) return tanhf(x); // Tanh pour valeurs n√©gatives
            else return 0.5f * (x + tanhf(x)); // M√©lange pour [-1,1]
    }
}

// D√©riv√©e de l'activation
static float activation_derivative(float output, activation_type_t type) {
    switch (type) {
        case ACTIVATION_RELU:
            return (output > 0) ? 1.0f : 0.0f;
        case ACTIVATION_SIGMOID:
            return output * (1.0f - output);
        case ACTIVATION_TANH:
            return 1.0f - output * output;
        case ACTIVATION_LEAKY_RELU:
            return (output > 0) ? 1.0f : 0.01f;
        case ACTIVATION_LINEAR:
            return 1.0f;
        // D√âRIV√âES DES NOUVELLES ACTIVATIONS
        case ACTIVATION_GELU:
            // D√©riv√©e approximative de GELU
            return 0.5f * (1.0f + tanhf(0.7978845608f * (output + 0.044715f * output * output * output)));
        case ACTIVATION_MISH:
            // D√©riv√©e approximative de Mish
            return tanhf(logf(1.0f + expf(fminf(output, 20.0f))));
        case ACTIVATION_SWISH:
            // D√©riv√©e de Swish: sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))
            if (output > 500.0f) return 1.0f;
            if (output < -500.0f) return 0.0f;
            float sig = 1.0f / (1.0f + expf(-output));
            return sig + output * sig * (1.0f - sig);
        case ACTIVATION_ELU:
            // D√©riv√©e de ELU: 1 si x > 0, sinon Œ± * e^x avec Œ± = 1.0
            return (output > 0) ? 1.0f : (output + 1.0f); // output = Œ±(e^x - 1), donc e^x = output + 1
        case ACTIVATION_PRELU:
            return (output > 0) ? 1.0f : 0.01f;
        default:
            // ACTIVATION_NEUROPLAST ou inconnue : d√©riv√©e adaptative
            if (output > 1.0f) return 1.0f; // D√©riv√©e ReLU
            else if (output < -1.0f) return 1.0f - output * output; // D√©riv√©e Tanh
            else return 0.5f * (1.0f + (1.0f - output * output)); // M√©lange
    }
}

NeuralNetwork *network_create_simple(size_t n_layers, const size_t *layer_sizes, const char **activations) {
    if (n_layers < 2) {
        printf("Erreur: un r√©seau doit avoir au moins 2 couches\n");
        return NULL;
    }
    
    SimpleNeuralNetwork *net = malloc(sizeof(SimpleNeuralNetwork));
    if (!net) return NULL;
    
    net->num_layers = n_layers - 1;
    
    // Configuration des hyperparam√®tres pour convergence optimale
    net->learning_rate = 0.001f;  // LR de base standard
    net->momentum = 0.9f;         // Momentum standard
    net->use_momentum = 1;        // Activer momentum par d√©faut
    
    // ‚úÖ PARAM√àTRES DE R√âGULARISATION OPTIMIS√âS pour convergence stable
    net->class_weights[0] = 1.0f;   // Classe 0 (sain)
    net->class_weights[1] = 1.0f;   // Classe 1 (malade) - pas de d√©s√©quilibre par d√©faut
    net->dropout_rate = 0.0f;       // ‚úÖ Pas de dropout par d√©faut (peut √™tre activ√© s√©par√©ment)
    net->l2_lambda = 0.0001f;       // ‚úÖ R√©gularisation L2 tr√®s l√©g√®re (√©tait 0.0005 - trop fort)
    net->optimal_threshold = 0.5f;  // Seuil standard
    net->use_dropout = 0;           // Dropout d√©sactiv√© par d√©faut
    net->dropout_mask = NULL;
    
    // Allocation des couches
    net->layers = malloc(net->num_layers * sizeof(Layer*));
    if (!net->layers) {
        free(net);
        return NULL;
    }
    
    // Allocation des tableaux de momentum
    net->momentum_weights = malloc(net->num_layers * sizeof(float*));
    net->momentum_biases = malloc(net->num_layers * sizeof(float));
    
    // Cr√©ation des couches avec activations optimis√©es
    for (size_t i = 0; i < net->num_layers; i++) {
        size_t input_size = layer_sizes[i];
        size_t output_size = layer_sizes[i + 1];
        
        // S√©lection d'activation selon la configuration (toutes les activations support√©es)
        activation_type_t activation;
        if (i == net->num_layers - 1) {
            // Couche de sortie : sigmoid pour classification binaire
            activation = ACTIVATION_SIGMOID;
        } else if (strcmp(activations[i], "relu") == 0) {
            activation = ACTIVATION_RELU;
        } else if (strcmp(activations[i], "tanh") == 0) {
            activation = ACTIVATION_TANH;
        } else if (strcmp(activations[i], "sigmoid") == 0) {
            activation = ACTIVATION_SIGMOID;
        } else if (strcmp(activations[i], "leaky_relu") == 0) {
            activation = ACTIVATION_LEAKY_RELU;
        } else if (strcmp(activations[i], "gelu") == 0) {
            activation = ACTIVATION_GELU;
        } else if (strcmp(activations[i], "mish") == 0) {
            activation = ACTIVATION_MISH;
        } else if (strcmp(activations[i], "swish") == 0) {
            activation = ACTIVATION_SWISH;
        } else if (strcmp(activations[i], "elu") == 0) {
            activation = ACTIVATION_ELU;
        } else if (strcmp(activations[i], "prelu") == 0) {
            activation = ACTIVATION_PRELU;
        } else if (strcmp(activations[i], "neuroplast") == 0) {
            activation = ACTIVATION_NEUROPLAST; // Sera trait√© dans default du switch
        } else {
            // Par d√©faut : ReLU pour activations non reconnues
            activation = ACTIVATION_RELU;
        }
        
        net->layers[i] = layer_create(input_size, output_size, activation);
        if (!net->layers[i]) {
            // Nettoyage en cas d'erreur
            for (size_t j = 0; j < i; j++) {
                layer_free(net->layers[j]);
            }
            free(net->layers);
            free(net);
            return NULL;
        }
        
        // Initialisation des poids selon l'activation
        init_weights_simple(net->layers[i], activation);
        
        // OPTIMISATION SP√âCIALE pour la couche de sortie
        if (i == net->num_layers - 1) {
            // Biais n√©gatif pour favoriser la d√©tection des maladies cardiaques (rappel √©lev√©)
            net->layers[i]->biases[0] = -1.0f; // Logit pour probabilit√© ~27% (favorise d√©tection)
        }
        
        // Allocation momentum pour cette couche
        net->momentum_weights[i] = calloc(output_size * input_size, sizeof(float));
        
        printf("Couche %zu: %zu ‚Üí %zu (%s)\n", 
               i, input_size, output_size, activations[i]);
    }
    
    printf("R√©seau simple cr√©√© avec succ√®s (%zu couches)\n", net->num_layers);
    printf("‚úÖ Class weights: [%.1f, %.1f] (sain, malade)\n", net->class_weights[0], net->class_weights[1]);
    printf("‚úÖ Learning rate: %.4f | Dropout: %.0f%% | L2: %.4f\n", 
           net->learning_rate, net->dropout_rate * 100, net->l2_lambda);
    printf("‚úÖ Seuil optimal: %.2f | Momentum: %.2f\n", 
           net->optimal_threshold, net->momentum);
    
    return (NeuralNetwork*)net;
}

// NOUVELLE FONCTION AVEC CONFIGURATION PERSONNALIS√âE
NeuralNetwork *network_create_simple_configured(size_t n_layers, const size_t *layer_sizes, 
                                                const char **activations, NetworkConfig config) {
    if (n_layers < 2) {
        printf("Erreur: un r√©seau doit avoir au moins 2 couches\n");
        return NULL;
    }
    
    SimpleNeuralNetwork *net = malloc(sizeof(SimpleNeuralNetwork));
    if (!net) return NULL;
    
    net->num_layers = n_layers - 1;
    
    // UTILISATION DE LA CONFIGURATION PERSONNALIS√âE
    net->learning_rate = config.learning_rate;
    net->momentum = config.momentum;
    net->use_momentum = config.use_momentum;
    
    // √âQUILIBRAGE DES CLASSES CONFIGURABLE
    net->class_weights[0] = 1.0f;                      // Classe majoritaire : toujours 1.0
    net->class_weights[1] = config.class_weight_ratio; // Classe minoritaire configurable
    
    // R√âGULARISATION CONFIGURABLE
    net->dropout_rate = config.dropout_rate;
    net->l2_lambda = config.l2_lambda;
    net->optimal_threshold = config.optimal_threshold;
    net->use_dropout = config.use_dropout;
    
    // Allocation des couches
    net->layers = malloc(net->num_layers * sizeof(Layer*));
    if (!net->layers) {
        free(net);
        return NULL;
    }
    
    // Allocation des tableaux de momentum
    net->momentum_weights = malloc(net->num_layers * sizeof(float*));
    net->momentum_biases = malloc(net->num_layers * sizeof(float));
    
    // Allocation du masque de dropout pour la couche cach√©e
    if (n_layers > 2) {
        net->dropout_mask = malloc(layer_sizes[1] * sizeof(float)); // Couche cach√©e
    } else {
        net->dropout_mask = NULL;
    }
    
    // Cr√©ation des couches avec activations configur√©es
    for (size_t i = 0; i < net->num_layers; i++) {
        size_t input_size = layer_sizes[i];
        size_t output_size = layer_sizes[i + 1];
        
        // S√©lection d'activation selon la configuration (toutes les activations support√©es)
        activation_type_t activation;
        if (i == net->num_layers - 1) {
            // Couche de sortie : sigmoid pour classification binaire
            activation = ACTIVATION_SIGMOID;
        } else if (strcmp(activations[i], "relu") == 0) {
            activation = ACTIVATION_RELU;
        } else if (strcmp(activations[i], "tanh") == 0) {
            activation = ACTIVATION_TANH;
        } else if (strcmp(activations[i], "sigmoid") == 0) {
            activation = ACTIVATION_SIGMOID;
        } else if (strcmp(activations[i], "leaky_relu") == 0) {
            activation = ACTIVATION_LEAKY_RELU;
        } else if (strcmp(activations[i], "gelu") == 0) {
            activation = ACTIVATION_GELU;
        } else if (strcmp(activations[i], "mish") == 0) {
            activation = ACTIVATION_MISH;
        } else if (strcmp(activations[i], "swish") == 0) {
            activation = ACTIVATION_SWISH;
        } else if (strcmp(activations[i], "elu") == 0) {
            activation = ACTIVATION_ELU;
        } else if (strcmp(activations[i], "prelu") == 0) {
            activation = ACTIVATION_PRELU;
        } else if (strcmp(activations[i], "neuroplast") == 0) {
            activation = ACTIVATION_NEUROPLAST; // Sera trait√© dans default du switch
        } else {
            // Par d√©faut : ReLU pour activations non reconnues
            activation = ACTIVATION_RELU;
        }
        
        net->layers[i] = layer_create(input_size, output_size, activation);
        if (!net->layers[i]) {
            // Nettoyage en cas d'erreur
            for (size_t j = 0; j < i; j++) {
                layer_free(net->layers[j]);
            }
            free(net->layers);
            free(net);
            return NULL;
        }
        
        // Initialisation des poids selon l'activation
        init_weights_simple(net->layers[i], activation);
        
        // Allocation momentum pour cette couche
        net->momentum_weights[i] = calloc(output_size * input_size, sizeof(float));
        
        printf("Couche %zu: %zu ‚Üí %zu (%s)\n", 
               i, input_size, output_size, activations[i]);
    }
    
    printf("R√©seau configur√© cr√©√© avec succ√®s (%zu couches)\n", net->num_layers);
    printf("‚úÖ Class weights: [%.1f, %.1f] | LR: %.4f | Momentum: %.2f\n", 
           net->class_weights[0], net->class_weights[1], net->learning_rate, net->momentum);
    printf("‚úÖ Dropout: %.0f%% | L2: %.4f | Seuil: %.2f\n", 
           net->dropout_rate * 100, net->l2_lambda, net->optimal_threshold);
    
    return (NeuralNetwork*)net;
}

void network_forward_simple(NeuralNetwork *net, float *input) {
    SimpleNeuralNetwork *simple_net = (SimpleNeuralNetwork*)net;
    
    float *current_input = input;
    
    for (size_t i = 0; i < simple_net->num_layers; i++) {
        Layer *layer = simple_net->layers[i];
        
        // Calcul direct : z = W*x + b, puis activation
        for (size_t j = 0; j < layer->output_size; j++) {
            float z = layer->biases[j];
            
            for (size_t k = 0; k < layer->input_size; k++) {
                z += layer->weights[j][k] * current_input[k];
            }
            
            // Application de l'activation
            layer->outputs[j] = apply_activation(z, layer->activation_type);
        }
        
        // Appliquer dropout si activ√© (sauf pour la couche de sortie)
        if (simple_net->use_dropout && simple_net->dropout_mask && (size_t)i < simple_net->num_layers - 1) {
            for (size_t n = 0; n < layer->output_size; n++) {
                // G√©n√©rer masque de dropout pour cette couche
                float dropout_prob = (float)rand() / RAND_MAX;
                if (dropout_prob < simple_net->dropout_rate) {
                    layer->outputs[n] = 0.0f;
                } else {
                    layer->outputs[n] /= (1.0f - simple_net->dropout_rate);
                }
            }
        }
        
        current_input = layer->outputs;
    }
}

void network_backward_simple(NeuralNetwork *net, float *input, float *target, float learning_rate) {
    SimpleNeuralNetwork *simple_net = (SimpleNeuralNetwork*)net;
    
    // Calcul de l'erreur pour la couche de sortie avec √©quilibrage des classes OPTIMIS√â
    Layer *output_layer = simple_net->layers[simple_net->num_layers - 1];
    for (size_t i = 0; i < output_layer->output_size; i++) {
        // Calculer l'erreur pour cette sortie
        float target_val = target[i];
        float output = output_layer->outputs[i];
        float error = target_val - output;
        
        // √âQUILIBRAGE DES CLASSES ADAPTATIF
        int target_class = (target_val > 0.5f) ? 1 : 0;
        float base_class_weight = simple_net->class_weights[target_class];
        
        // Pond√©ration adaptative : plus de poids pour les erreurs importantes
        float adaptive_weight = base_class_weight;
        
        // Ajustement selon la difficult√© de la pr√©diction
        if (target_val > 0.5f && output < 0.3f) {
            adaptive_weight *= 1.8f; // Cas tr√®s difficile : vrai positif mal pr√©dit
        } else if (target_val < 0.5f && output > 0.7f) {
            adaptive_weight *= 1.5f; // Cas difficile : faux positif
        } else if (fabsf(target_val - output) > 0.7f) {
            adaptive_weight *= 1.3f; // Erreur importante
        }
        
        // Delta = erreur pond√©r√©e * d√©riv√©e de l'activation avec stabilisation
        float derivative = activation_derivative(output, output_layer->activation_type);
        // Stabilisation pour √©viter les gradients √©vanescents dans sigmoid
        if (output_layer->activation_type == ACTIVATION_SIGMOID) {
            derivative = fmaxf(derivative, 0.01f); // Minimum 1% de gradient
        }
        
        output_layer->deltas[i] = error * adaptive_weight * derivative;
    }
    
    // R√©tropropagation pour les couches cach√©es
    for (int l = simple_net->num_layers - 2; l >= 0; l--) {
        Layer *current_layer = simple_net->layers[l];
        Layer *next_layer = simple_net->layers[l + 1];
        
        for (size_t i = 0; i < current_layer->output_size; i++) {
            float error = 0.0f;
            
            // Somme pond√©r√©e des erreurs de la couche suivante
            for (size_t j = 0; j < next_layer->output_size; j++) {
                error += next_layer->deltas[j] * next_layer->weights[j][i];
            }
            
            // Prise en compte du dropout dans la r√©tropropagation
            if (simple_net->use_dropout && simple_net->dropout_mask && (size_t)l < simple_net->num_layers - 1) {
                error *= simple_net->dropout_mask[i];
            }
            
            // Delta = erreur * d√©riv√©e
            float derivative = activation_derivative(current_layer->outputs[i], 
                                                   current_layer->activation_type);
            current_layer->deltas[i] = error * derivative;
        }
    }
    
    // Mise √† jour des poids avec SGD + momentum + r√©gularisation L2 OPTIMIS√âE
    float *layer_input = input;
    float effective_lr = learning_rate > 0 ? learning_rate : simple_net->learning_rate;
    
    // Gradient clipping mod√©r√© pour stabilit√© sans bloquer l'apprentissage
    float max_gradient_norm = 10.0f; // ‚úÖ Augment√© de 3.0 √† 10.0 pour permettre l'apprentissage
    
    for (size_t l = 0; l < simple_net->num_layers; l++) {
        Layer *layer = simple_net->layers[l];
        
        // Calcul de la norme des gradients pour cette couche
        float gradient_norm = 0.0f;
        for (size_t i = 0; i < layer->output_size; i++) {
            for (size_t j = 0; j < layer->input_size; j++) {
                float grad = layer->deltas[i] * layer_input[j];
                gradient_norm += grad * grad;
            }
            gradient_norm += layer->deltas[i] * layer->deltas[i]; // Biais aussi
        }
        gradient_norm = sqrtf(gradient_norm);
        
        // Facteur de clipping si n√©cessaire (plus conservateur)
        float clip_factor = 1.0f;
        if (gradient_norm > max_gradient_norm) {
            clip_factor = max_gradient_norm / gradient_norm;
        }
        
        // Learning rate standard (pas d'adaptation par couche automatique)
        float layer_lr = effective_lr;
        
        for (size_t i = 0; i < layer->output_size; i++) {
            // Mise √† jour des poids
            for (size_t j = 0; j < layer->input_size; j++) {
                float gradient = layer->deltas[i] * layer_input[j] * clip_factor;
                
                // R√âGULARISATION L2 mod√©r√©e : ajout du terme de p√©nalit√©
                float l2_penalty = simple_net->l2_lambda * layer->weights[i][j];
                gradient += l2_penalty;
                
                if (simple_net->use_momentum) {
                    // SGD avec momentum optimis√©
                    size_t idx = i * layer->input_size + j;
                    simple_net->momentum_weights[l][idx] = 
                        simple_net->momentum * simple_net->momentum_weights[l][idx] + 
                        layer_lr * gradient;
                    layer->weights[i][j] += simple_net->momentum_weights[l][idx];
                } else {
                    // SGD simple avec L2
                    layer->weights[i][j] += layer_lr * gradient;
                }
            }
            
            // Mise √† jour des biais avec clipping
            float bias_gradient = layer->deltas[i] * clip_factor;
            layer->biases[i] += layer_lr * bias_gradient;
        }
        
        layer_input = layer->outputs;
    }
}

void network_free_simple(NeuralNetwork *net) {
    SimpleNeuralNetwork *simple_net = (SimpleNeuralNetwork*)net;
    if (!simple_net) return;
    
    if (simple_net->layers) {
        for (size_t i = 0; i < simple_net->num_layers; i++) {
            layer_free(simple_net->layers[i]);
            if (simple_net->momentum_weights && simple_net->momentum_weights[i]) {
                free(simple_net->momentum_weights[i]);
            }
        }
        free(simple_net->layers);
    }
    
    if (simple_net->momentum_weights) free(simple_net->momentum_weights);
    if (simple_net->momentum_biases) free(simple_net->momentum_biases);
    if (simple_net->dropout_mask) free(simple_net->dropout_mask);
    free(simple_net);
}

float *network_output_simple(NeuralNetwork *net) {
    SimpleNeuralNetwork *simple_net = (SimpleNeuralNetwork*)net;
    if (!simple_net || !simple_net->layers || simple_net->num_layers == 0) return NULL;
    return simple_net->layers[simple_net->num_layers - 1]->outputs;
}

// Fonction pour activer/d√©sactiver le dropout (entra√Ænement vs √©valuation)
void network_set_dropout_simple(NeuralNetwork *net, int use_dropout) {
    SimpleNeuralNetwork *simple_net = (SimpleNeuralNetwork*)net;
    simple_net->use_dropout = use_dropout;
}

// Fonction pour optimiser le seuil de d√©cision bas√© sur F1-score
float optimize_threshold_simple(NeuralNetwork *net, float inputs[][21], float targets[], int num_samples) {
    SimpleNeuralNetwork *simple_net = (SimpleNeuralNetwork*)net;
    
    // D√©sactiver dropout pour √©valuation
    int original_dropout = simple_net->use_dropout;
    simple_net->use_dropout = 0;
    
    float best_threshold = 0.5f;
    float best_f1 = 0.0f;
    
    printf("üîç Debug: Analyse des pr√©dictions avant optimisation...\n");
    
    // Analyser les pr√©dictions actuelles
    float min_score = 1.0f, max_score = 0.0f;
    int actual_positives = 0;
    for (int i = 0; i < num_samples; i++) {
        network_forward_simple(net, inputs[i]);
        float *output = network_output_simple(net);
        
        if (output) {
            float score = output[0];
            if (score < min_score) min_score = score;
            if (score > max_score) max_score = score;
            
            if (targets[i] > 0.5f) actual_positives++;
        }
    }
    
    printf("üìä Scores: min=%.4f, max=%.4f | Positifs r√©els: %d/%d\n", 
           min_score, max_score, actual_positives, num_samples);
    
    // Tester diff√©rents seuils avec recherche adaptative (plus efficace)
    
    // Phase 1: Recherche grossi√®re de 0.01 √† 0.99
    for (float threshold = 0.01f; threshold <= 0.99f; threshold += 0.01f) {
        int TP = 0, FP = 0, FN = 0, TN = 0;
        
        // Calculer m√©triques pour ce seuil
        for (int i = 0; i < num_samples; i++) {
            network_forward_simple(net, inputs[i]);
            float *output = network_output_simple(net);
            
            if (output) {
                int predicted = (output[0] > threshold) ? 1 : 0;
                int actual = (targets[i] > 0.5f) ? 1 : 0;
                
                if (predicted == 1 && actual == 1) TP++;
                else if (predicted == 1 && actual == 0) FP++;
                else if (predicted == 0 && actual == 1) FN++;
                else TN++;
            }
        }
        
        // Calculer m√©triques compl√®tes
        float precision = (TP + FP > 0) ? (float)TP / (TP + FP) : 0.0f;
        float recall = (TP + FN > 0) ? (float)TP / (TP + FN) : 0.0f;
        float f1 = (precision + recall > 0) ? 2.0f * precision * recall / (precision + recall) : 0.0f;
        
                 // M√©trique composite SP√âCIALIS√âE M√âDICAL pour maximiser le rappel
         // En m√©dical, mieux vaut trop d√©tecter que pas assez (priorit√© au rappel)
         float composite_score = 0.0f;
         if (recall >= 0.5f && precision >= 0.05f) { // Seuils minimums adapt√©s au m√©dical
             composite_score = 0.6f * recall + 0.3f * f1 + 0.1f * precision; // Priorit√© massive au rappel
         }
        
        // Debug pour les seuils int√©ressants
        if (threshold <= 0.15f || f1 > 0.1f || composite_score > 0.0f) {
            printf("  Seuil %.2f: TP=%d FP=%d FN=%d TN=%d | Prec=%.3f Recall=%.3f F1=%.3f Comp=%.3f\n",
                   threshold, TP, FP, FN, TN, precision, recall, f1, composite_score);
        }
        
        // Mettre √† jour le meilleur selon le score composite
        if (composite_score > best_f1) {
            best_f1 = composite_score;
            best_threshold = threshold;
        }
    }
    
    // Restaurer √©tat dropout
    simple_net->use_dropout = original_dropout;
    simple_net->optimal_threshold = best_threshold;
    
    printf("üéØ Seuil optimal trouv√©: %.2f (F1: %.3f)\n", best_threshold, best_f1);
    return best_threshold;
}

// Fonction pour faire des pr√©dictions avec le seuil optimal
int predict_with_optimal_threshold_simple(NeuralNetwork *net, float *input) {
    SimpleNeuralNetwork *simple_net = (SimpleNeuralNetwork*)net;
    
    // D√©sactiver dropout pour pr√©diction
    int original_dropout = simple_net->use_dropout;
    simple_net->use_dropout = 0;
    
    network_forward_simple(net, input);
    float *output = network_output_simple(net);
    
    // Restaurer dropout
    simple_net->use_dropout = original_dropout;
    
    if (!output) return -1;
    
    return (output[0] > simple_net->optimal_threshold) ? 1 : 0;
} 